{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bioinformatics - Update on 28 December 2021 @TarekHasan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5T-Ie9HYITy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class SeqEmbedding():\n",
        "    def __init__(self):\n",
        "        self.Dictionary = {}\n",
        "\n",
        "    def __Sequence_to_Numeric(self,k,sequence):\n",
        "        if k==0:\n",
        "            temp = [0] * self.size_of_vector\n",
        "            temp[len(self.Dictionary)] = 1\n",
        "            self.Dictionary[sequence] = temp\n",
        "            return\n",
        "        nucleotide = ['A','C','G','T']\n",
        "        for n in nucleotide:\n",
        "            self.__Sequence_to_Numeric(k-1,sequence+n)\n",
        "        return\n",
        "\n",
        "    def fit(self, sequences, window_size, stride_size):\n",
        "        self.size_of_vector = 4 ** window_size\n",
        "        self.__Sequence_to_Numeric(window_size,\"\")\n",
        "\n",
        "        vectorized = []\n",
        "\n",
        "        for seq in sequences:\n",
        "            first_layer_embedding = []\n",
        "            for k in range(window_size, len(seq)+1, stride_size):\n",
        "                try:\n",
        "                    first_layer_embedding.append(self.Dictionary[seq[k-window_size:k]])\n",
        "                except:\n",
        "                    # exception may occur because of stride size, sometimes it may not get sequence of length window size, there will be a key not found exception in Dictionary\n",
        "                    first_layer_embedding.append([0]*self.size_of_vector)\n",
        "            \n",
        "            vector0 = []\n",
        "            vector1 = []\n",
        "            for i in range(len(first_layer_embedding)):\n",
        "                if i>0:\n",
        "                    vector1+=first_layer_embedding[i]\n",
        "                vector0+=first_layer_embedding[i]\n",
        "            vector1+=first_layer_embedding[0]\n",
        "\n",
        "            vectorized.append([vector0, vector1])\n",
        "        \n",
        "        # Handling inequal length problem using zero padding\n",
        "        max_len = 0\n",
        "        for vec in vectorized:\n",
        "            max_len = max(max_len, len(vec[0]))\n",
        "        for i in range(len(vectorized)):\n",
        "            required = max_len - len(vectorized[i][0])\n",
        "\n",
        "            vectorized[i][0]+=([0]*required)\n",
        "            vectorized[i][0] = vectorized[i][0][:31752] # to fit into 252 * 252 pixel size, removing 120 elements\n",
        "            vectorized[i][0] = np.array(vectorized[i][0])\n",
        "\n",
        "            vectorized[i][1]+=([0]*required)\n",
        "            vectorized[i][1] = vectorized[i][1][:31752] # to fit into 252 * 252 pixel size, removing 120 elements\n",
        "            vectorized[i][1] = np.array(vectorized[i][1])\n",
        "\n",
        "            vectorized[i] = np.array(vectorized[i])\n",
        "            \n",
        "        return np.array(vectorized)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "ZH_UZDqQYQDX",
        "outputId": "53bd726c-9637-429d-d68e-a831ae5ed32e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/Introduction to Bioinformatics/Project Proposal/Dataset/histone/H3K4me2.txt\",\"r\")\n",
        "sequences = []\n",
        "class_label = []\n",
        "for line in file:\n",
        "    if '>' in line:\n",
        "        continue\n",
        "    line = line[:-1]\n",
        "    if len(line)>1:\n",
        "        sequences.append(line)\n",
        "    else:\n",
        "        class_label.append(int(line)) \n",
        "file.close()"
      ],
      "metadata": {
        "id": "COvh-5T2YU6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_sz = (5000.0/len(sequences))\n",
        "sequences, _sequences, class_label, _class_label = train_test_split(sequences, class_label, train_size = train_sz, random_state=42, stratify=class_label)"
      ],
      "metadata": {
        "id": "7hmD7EHkuz5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_label = np.array(class_label)"
      ],
      "metadata": {
        "id": "ugyhyuFgvCOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(sequences, class_label, test_size=0.2, random_state=42, stratify=class_label)"
      ],
      "metadata": {
        "id": "V4KCP0kFvFiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance = SeqEmbedding()\n",
        "X_train_val = instance.fit(sequences = X_train_val, window_size = 3, stride_size = 1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42, stratify = y_train_val)"
      ],
      "metadata": {
        "id": "M3FXYBuTvbPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK2QU98gBwPS",
        "outputId": "7fa3a19a-83c2-4253-e229-c5af246fc8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3600, 2, 31752)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "model = SVC(kernel = 'linear')\n",
        "model.fit(X_train.reshape(X_train.shape[0], -1),y_train)\n",
        "y_pred = model.predict(X_val.reshape(X_val.shape[0], -1))\n",
        "print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihqJ7TrixNls",
        "outputId": "47fb4b58-f2b9-4de6-a760-1a80b4d07148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.5791666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0],252, 252, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 252, 252, 1)"
      ],
      "metadata": {
        "id": "s7wCke3N5WTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsI9Q8gc5quD",
        "outputId": "687ca8f5-5fa4-402c-a378-5f5839a46f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2160, 252, 252, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "metadata": {
        "id": "L3MH7BaS5EZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(252,252,1)),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(100,activation='relu'),\n",
        "                                    tf.keras.layers.Dropout(0.5),\n",
        "                                    tf.keras.layers.Dense(2,activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEVpMiP35C68",
        "outputId": "fb3a22bc-100a-4c66-9975-2a43f21823ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 250, 250, 16)      160       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 125, 125, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 123, 123, 16)      2320      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 61, 61, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 59536)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               5953700   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,956,382\n",
            "Trainable params: 5,956,382\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OfcMubs6C2Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es = EarlyStopping(monitor='val_accuracy',mode='max',verbose=1,patience=200)\n",
        "mc = ModelCheckpoint('best_model.h5',monitor='val_accuracy',mode='max',verbose=1,save_best_only=True)\n",
        "\n",
        "hist = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=300,batch_size=512,callbacks=[es,mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTwdFedgGxCz",
        "outputId": "47cffce9-a9e9-488d-9fc2-c9390ccabde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.5495\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.59167, saving model to best_model.h5\n",
            "5/5 [==============================] - 19s 1s/step - loss: 0.6875 - accuracy: 0.5495 - val_loss: 0.6758 - val_accuracy: 0.5917\n",
            "Epoch 2/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6794 - accuracy: 0.5889\n",
            "Epoch 00002: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.6794 - accuracy: 0.5889 - val_loss: 0.6757 - val_accuracy: 0.5917\n",
            "Epoch 3/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.5921\n",
            "Epoch 00003: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.6796 - accuracy: 0.5921 - val_loss: 0.6750 - val_accuracy: 0.5917\n",
            "Epoch 4/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.5907\n",
            "Epoch 00004: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.6776 - accuracy: 0.5907 - val_loss: 0.6759 - val_accuracy: 0.5917\n",
            "Epoch 5/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.5907\n",
            "Epoch 00005: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.6777 - accuracy: 0.5907 - val_loss: 0.6751 - val_accuracy: 0.5917\n",
            "Epoch 6/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.5889\n",
            "Epoch 00006: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 593ms/step - loss: 0.6737 - accuracy: 0.5889 - val_loss: 0.6767 - val_accuracy: 0.5917\n",
            "Epoch 7/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6736 - accuracy: 0.5912\n",
            "Epoch 00007: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.6736 - accuracy: 0.5912 - val_loss: 0.6771 - val_accuracy: 0.5917\n",
            "Epoch 8/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6732 - accuracy: 0.5958\n",
            "Epoch 00008: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6732 - accuracy: 0.5958 - val_loss: 0.6753 - val_accuracy: 0.5917\n",
            "Epoch 9/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.5894\n",
            "Epoch 00009: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 594ms/step - loss: 0.6715 - accuracy: 0.5894 - val_loss: 0.6752 - val_accuracy: 0.5917\n",
            "Epoch 10/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6694 - accuracy: 0.5935\n",
            "Epoch 00010: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.6694 - accuracy: 0.5935 - val_loss: 0.6767 - val_accuracy: 0.5917\n",
            "Epoch 11/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.5944\n",
            "Epoch 00011: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 593ms/step - loss: 0.6698 - accuracy: 0.5944 - val_loss: 0.6742 - val_accuracy: 0.5917\n",
            "Epoch 12/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.5963\n",
            "Epoch 00012: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.6696 - accuracy: 0.5963 - val_loss: 0.6745 - val_accuracy: 0.5917\n",
            "Epoch 13/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.5903\n",
            "Epoch 00013: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.6680 - accuracy: 0.5903 - val_loss: 0.6755 - val_accuracy: 0.5917\n",
            "Epoch 14/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.5921\n",
            "Epoch 00014: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.6662 - accuracy: 0.5921 - val_loss: 0.6747 - val_accuracy: 0.5917\n",
            "Epoch 15/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6655 - accuracy: 0.5931\n",
            "Epoch 00015: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 592ms/step - loss: 0.6655 - accuracy: 0.5931 - val_loss: 0.6753 - val_accuracy: 0.5917\n",
            "Epoch 16/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6603 - accuracy: 0.5991\n",
            "Epoch 00016: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 594ms/step - loss: 0.6603 - accuracy: 0.5991 - val_loss: 0.6752 - val_accuracy: 0.5917\n",
            "Epoch 17/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6609 - accuracy: 0.5972\n",
            "Epoch 00017: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.6609 - accuracy: 0.5972 - val_loss: 0.6780 - val_accuracy: 0.5917\n",
            "Epoch 18/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6621 - accuracy: 0.5940\n",
            "Epoch 00018: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.6621 - accuracy: 0.5940 - val_loss: 0.6750 - val_accuracy: 0.5917\n",
            "Epoch 19/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6616 - accuracy: 0.5921\n",
            "Epoch 00019: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.6616 - accuracy: 0.5921 - val_loss: 0.6750 - val_accuracy: 0.5875\n",
            "Epoch 20/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6614 - accuracy: 0.5991\n",
            "Epoch 00020: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.6614 - accuracy: 0.5991 - val_loss: 0.6753 - val_accuracy: 0.5875\n",
            "Epoch 21/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.6009\n",
            "Epoch 00021: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.6579 - accuracy: 0.6009 - val_loss: 0.6747 - val_accuracy: 0.5917\n",
            "Epoch 22/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6586 - accuracy: 0.5986\n",
            "Epoch 00022: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6586 - accuracy: 0.5986 - val_loss: 0.6739 - val_accuracy: 0.5917\n",
            "Epoch 23/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.5981\n",
            "Epoch 00023: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.6551 - accuracy: 0.5981 - val_loss: 0.6743 - val_accuracy: 0.5917\n",
            "Epoch 24/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.5954\n",
            "Epoch 00024: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6556 - accuracy: 0.5954 - val_loss: 0.6739 - val_accuracy: 0.5917\n",
            "Epoch 25/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.5991\n",
            "Epoch 00025: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.6535 - accuracy: 0.5991 - val_loss: 0.6735 - val_accuracy: 0.5917\n",
            "Epoch 26/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.5991\n",
            "Epoch 00026: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6519 - accuracy: 0.5991 - val_loss: 0.6738 - val_accuracy: 0.5917\n",
            "Epoch 27/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6480 - accuracy: 0.6037\n",
            "Epoch 00027: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.6480 - accuracy: 0.6037 - val_loss: 0.6745 - val_accuracy: 0.5875\n",
            "Epoch 28/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.6120\n",
            "Epoch 00028: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.6484 - accuracy: 0.6120 - val_loss: 0.6748 - val_accuracy: 0.5917\n",
            "Epoch 29/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6464 - accuracy: 0.6079\n",
            "Epoch 00029: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.6464 - accuracy: 0.6079 - val_loss: 0.6735 - val_accuracy: 0.5917\n",
            "Epoch 30/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6429 - accuracy: 0.6130\n",
            "Epoch 00030: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.6429 - accuracy: 0.6130 - val_loss: 0.6735 - val_accuracy: 0.5917\n",
            "Epoch 31/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6454 - accuracy: 0.6083\n",
            "Epoch 00031: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.6454 - accuracy: 0.6083 - val_loss: 0.6732 - val_accuracy: 0.5875\n",
            "Epoch 32/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6434 - accuracy: 0.6116\n",
            "Epoch 00032: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.6434 - accuracy: 0.6116 - val_loss: 0.6734 - val_accuracy: 0.5875\n",
            "Epoch 33/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6411 - accuracy: 0.6213\n",
            "Epoch 00033: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6411 - accuracy: 0.6213 - val_loss: 0.6731 - val_accuracy: 0.5917\n",
            "Epoch 34/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6434 - accuracy: 0.6093\n",
            "Epoch 00034: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6434 - accuracy: 0.6093 - val_loss: 0.6741 - val_accuracy: 0.5917\n",
            "Epoch 35/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.6144\n",
            "Epoch 00035: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.6370 - accuracy: 0.6144 - val_loss: 0.6725 - val_accuracy: 0.5917\n",
            "Epoch 36/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6373 - accuracy: 0.6162\n",
            "Epoch 00036: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.6373 - accuracy: 0.6162 - val_loss: 0.6738 - val_accuracy: 0.5875\n",
            "Epoch 37/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.6227\n",
            "Epoch 00037: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.6371 - accuracy: 0.6227 - val_loss: 0.6720 - val_accuracy: 0.5917\n",
            "Epoch 38/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.6222\n",
            "Epoch 00038: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.6358 - accuracy: 0.6222 - val_loss: 0.6730 - val_accuracy: 0.5917\n",
            "Epoch 39/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.6199\n",
            "Epoch 00039: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.6337 - accuracy: 0.6199 - val_loss: 0.6717 - val_accuracy: 0.5875\n",
            "Epoch 40/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.6222\n",
            "Epoch 00040: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.6305 - accuracy: 0.6222 - val_loss: 0.6713 - val_accuracy: 0.5875\n",
            "Epoch 41/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6266 - accuracy: 0.6329\n",
            "Epoch 00041: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.6266 - accuracy: 0.6329 - val_loss: 0.6752 - val_accuracy: 0.5875\n",
            "Epoch 42/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.6528\n",
            "Epoch 00042: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.6255 - accuracy: 0.6528 - val_loss: 0.6747 - val_accuracy: 0.5875\n",
            "Epoch 43/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.6644\n",
            "Epoch 00043: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.6269 - accuracy: 0.6644 - val_loss: 0.6736 - val_accuracy: 0.5917\n",
            "Epoch 44/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.6370\n",
            "Epoch 00044: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.6243 - accuracy: 0.6370 - val_loss: 0.6766 - val_accuracy: 0.5917\n",
            "Epoch 45/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.6398\n",
            "Epoch 00045: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.6203 - accuracy: 0.6398 - val_loss: 0.6717 - val_accuracy: 0.5792\n",
            "Epoch 46/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.6625\n",
            "Epoch 00046: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.6179 - accuracy: 0.6625 - val_loss: 0.6719 - val_accuracy: 0.5792\n",
            "Epoch 47/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.6829\n",
            "Epoch 00047: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.6145 - accuracy: 0.6829 - val_loss: 0.6745 - val_accuracy: 0.5917\n",
            "Epoch 48/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6173 - accuracy: 0.6611\n",
            "Epoch 00048: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.6173 - accuracy: 0.6611 - val_loss: 0.6724 - val_accuracy: 0.5833\n",
            "Epoch 49/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6127 - accuracy: 0.6704\n",
            "Epoch 00049: val_accuracy did not improve from 0.59167\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.6127 - accuracy: 0.6704 - val_loss: 0.6725 - val_accuracy: 0.5875\n",
            "Epoch 50/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6117 - accuracy: 0.6694\n",
            "Epoch 00050: val_accuracy improved from 0.59167 to 0.60833, saving model to best_model.h5\n",
            "5/5 [==============================] - 3s 615ms/step - loss: 0.6117 - accuracy: 0.6694 - val_loss: 0.6747 - val_accuracy: 0.6083\n",
            "Epoch 51/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.6773\n",
            "Epoch 00051: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.6093 - accuracy: 0.6773 - val_loss: 0.6723 - val_accuracy: 0.5917\n",
            "Epoch 52/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.6843\n",
            "Epoch 00052: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.6093 - accuracy: 0.6843 - val_loss: 0.6708 - val_accuracy: 0.5792\n",
            "Epoch 53/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.6755\n",
            "Epoch 00053: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.6045 - accuracy: 0.6755 - val_loss: 0.6713 - val_accuracy: 0.5792\n",
            "Epoch 54/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.6833\n",
            "Epoch 00054: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.6017 - accuracy: 0.6833 - val_loss: 0.6718 - val_accuracy: 0.5958\n",
            "Epoch 55/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.6852\n",
            "Epoch 00055: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.6000 - accuracy: 0.6852 - val_loss: 0.6746 - val_accuracy: 0.5917\n",
            "Epoch 56/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5973 - accuracy: 0.6894\n",
            "Epoch 00056: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.5973 - accuracy: 0.6894 - val_loss: 0.6764 - val_accuracy: 0.5917\n",
            "Epoch 57/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5932 - accuracy: 0.7023\n",
            "Epoch 00057: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.5932 - accuracy: 0.7023 - val_loss: 0.6901 - val_accuracy: 0.5125\n",
            "Epoch 58/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5977 - accuracy: 0.6954\n",
            "Epoch 00058: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.5977 - accuracy: 0.6954 - val_loss: 0.6881 - val_accuracy: 0.5917\n",
            "Epoch 59/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.6954\n",
            "Epoch 00059: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.5931 - accuracy: 0.6954 - val_loss: 0.6721 - val_accuracy: 0.5917\n",
            "Epoch 60/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.7032\n",
            "Epoch 00060: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.5859 - accuracy: 0.7032 - val_loss: 0.6720 - val_accuracy: 0.5958\n",
            "Epoch 61/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.7255\n",
            "Epoch 00061: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.5803 - accuracy: 0.7255 - val_loss: 0.6811 - val_accuracy: 0.5917\n",
            "Epoch 62/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.7097\n",
            "Epoch 00062: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.5823 - accuracy: 0.7097 - val_loss: 0.6753 - val_accuracy: 0.5875\n",
            "Epoch 63/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.7245\n",
            "Epoch 00063: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5752 - accuracy: 0.7245 - val_loss: 0.6730 - val_accuracy: 0.6042\n",
            "Epoch 64/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5795 - accuracy: 0.7333\n",
            "Epoch 00064: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.5795 - accuracy: 0.7333 - val_loss: 0.6850 - val_accuracy: 0.5917\n",
            "Epoch 65/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5749 - accuracy: 0.7278\n",
            "Epoch 00065: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.5749 - accuracy: 0.7278 - val_loss: 0.6977 - val_accuracy: 0.5917\n",
            "Epoch 66/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5867 - accuracy: 0.7102\n",
            "Epoch 00066: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.5867 - accuracy: 0.7102 - val_loss: 0.7008 - val_accuracy: 0.4625\n",
            "Epoch 67/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.7278\n",
            "Epoch 00067: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.5771 - accuracy: 0.7278 - val_loss: 0.6752 - val_accuracy: 0.5917\n",
            "Epoch 68/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.7463\n",
            "Epoch 00068: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.5641 - accuracy: 0.7463 - val_loss: 0.7004 - val_accuracy: 0.5917\n",
            "Epoch 69/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7125\n",
            "Epoch 00069: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.5748 - accuracy: 0.7125 - val_loss: 0.6849 - val_accuracy: 0.5833\n",
            "Epoch 70/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7407\n",
            "Epoch 00070: val_accuracy did not improve from 0.60833\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.5589 - accuracy: 0.7407 - val_loss: 0.6889 - val_accuracy: 0.5833\n",
            "Epoch 71/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5547 - accuracy: 0.7532\n",
            "Epoch 00071: val_accuracy improved from 0.60833 to 0.62083, saving model to best_model.h5\n",
            "5/5 [==============================] - 3s 647ms/step - loss: 0.5547 - accuracy: 0.7532 - val_loss: 0.6764 - val_accuracy: 0.6208\n",
            "Epoch 72/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.7486\n",
            "Epoch 00072: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.5525 - accuracy: 0.7486 - val_loss: 0.6885 - val_accuracy: 0.5875\n",
            "Epoch 73/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7282\n",
            "Epoch 00073: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.5645 - accuracy: 0.7282 - val_loss: 0.6753 - val_accuracy: 0.6125\n",
            "Epoch 74/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.7685\n",
            "Epoch 00074: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.5447 - accuracy: 0.7685 - val_loss: 0.6777 - val_accuracy: 0.6125\n",
            "Epoch 75/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.7681\n",
            "Epoch 00075: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 617ms/step - loss: 0.5478 - accuracy: 0.7681 - val_loss: 0.6843 - val_accuracy: 0.5833\n",
            "Epoch 76/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7190\n",
            "Epoch 00076: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.5681 - accuracy: 0.7190 - val_loss: 0.6908 - val_accuracy: 0.5833\n",
            "Epoch 77/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.7454\n",
            "Epoch 00077: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5510 - accuracy: 0.7454 - val_loss: 0.6778 - val_accuracy: 0.6042\n",
            "Epoch 78/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7731\n",
            "Epoch 00078: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.5347 - accuracy: 0.7731 - val_loss: 0.7077 - val_accuracy: 0.5875\n",
            "Epoch 79/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.7667\n",
            "Epoch 00079: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5317 - accuracy: 0.7667 - val_loss: 0.6836 - val_accuracy: 0.6000\n",
            "Epoch 80/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.7676\n",
            "Epoch 00080: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.5292 - accuracy: 0.7676 - val_loss: 0.7204 - val_accuracy: 0.4625\n",
            "Epoch 81/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.7208\n",
            "Epoch 00081: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.5590 - accuracy: 0.7208 - val_loss: 0.6794 - val_accuracy: 0.6083\n",
            "Epoch 82/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.7736\n",
            "Epoch 00082: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.5245 - accuracy: 0.7736 - val_loss: 0.6833 - val_accuracy: 0.6000\n",
            "Epoch 83/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.7931\n",
            "Epoch 00083: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5175 - accuracy: 0.7931 - val_loss: 0.6792 - val_accuracy: 0.6042\n",
            "Epoch 84/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.7944\n",
            "Epoch 00084: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5155 - accuracy: 0.7944 - val_loss: 0.7072 - val_accuracy: 0.5833\n",
            "Epoch 85/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.7477\n",
            "Epoch 00085: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.5280 - accuracy: 0.7477 - val_loss: 0.6869 - val_accuracy: 0.5833\n",
            "Epoch 86/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.7546\n",
            "Epoch 00086: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.5272 - accuracy: 0.7546 - val_loss: 0.7244 - val_accuracy: 0.5833\n",
            "Epoch 87/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.7356\n",
            "Epoch 00087: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.5394 - accuracy: 0.7356 - val_loss: 0.7749 - val_accuracy: 0.4417\n",
            "Epoch 88/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.7056\n",
            "Epoch 00088: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.5503 - accuracy: 0.7056 - val_loss: 0.7674 - val_accuracy: 0.5917\n",
            "Epoch 89/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.7426\n",
            "Epoch 00089: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5358 - accuracy: 0.7426 - val_loss: 0.6868 - val_accuracy: 0.6167\n",
            "Epoch 90/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.8056\n",
            "Epoch 00090: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.4934 - accuracy: 0.8056 - val_loss: 0.6879 - val_accuracy: 0.6042\n",
            "Epoch 91/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.8157\n",
            "Epoch 00091: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.4808 - accuracy: 0.8157 - val_loss: 0.7649 - val_accuracy: 0.5958\n",
            "Epoch 92/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.7384\n",
            "Epoch 00092: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.5310 - accuracy: 0.7384 - val_loss: 0.7004 - val_accuracy: 0.5292\n",
            "Epoch 93/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.7292\n",
            "Epoch 00093: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.5329 - accuracy: 0.7292 - val_loss: 0.7178 - val_accuracy: 0.5833\n",
            "Epoch 94/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.8106\n",
            "Epoch 00094: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.4808 - accuracy: 0.8106 - val_loss: 0.6968 - val_accuracy: 0.6125\n",
            "Epoch 95/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.7894\n",
            "Epoch 00095: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.4885 - accuracy: 0.7894 - val_loss: 0.7681 - val_accuracy: 0.4542\n",
            "Epoch 96/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.7292\n",
            "Epoch 00096: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.5296 - accuracy: 0.7292 - val_loss: 0.7567 - val_accuracy: 0.5875\n",
            "Epoch 97/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.7449\n",
            "Epoch 00097: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.5230 - accuracy: 0.7449 - val_loss: 0.6943 - val_accuracy: 0.6000\n",
            "Epoch 98/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.8250\n",
            "Epoch 00098: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.4664 - accuracy: 0.8250 - val_loss: 0.7690 - val_accuracy: 0.5792\n",
            "Epoch 99/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.7079\n",
            "Epoch 00099: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.5429 - accuracy: 0.7079 - val_loss: 0.7004 - val_accuracy: 0.5583\n",
            "Epoch 100/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.7907\n",
            "Epoch 00100: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.4829 - accuracy: 0.7907 - val_loss: 0.7961 - val_accuracy: 0.5833\n",
            "Epoch 101/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.7190\n",
            "Epoch 00101: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.5254 - accuracy: 0.7190 - val_loss: 0.7219 - val_accuracy: 0.4833\n",
            "Epoch 102/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5027 - accuracy: 0.7676\n",
            "Epoch 00102: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.5027 - accuracy: 0.7676 - val_loss: 0.7395 - val_accuracy: 0.5875\n",
            "Epoch 103/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.7722\n",
            "Epoch 00103: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.4936 - accuracy: 0.7722 - val_loss: 0.7620 - val_accuracy: 0.4583\n",
            "Epoch 104/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.7514\n",
            "Epoch 00104: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.5040 - accuracy: 0.7514 - val_loss: 0.7221 - val_accuracy: 0.6042\n",
            "Epoch 105/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8426\n",
            "Epoch 00105: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.4469 - accuracy: 0.8426 - val_loss: 0.7003 - val_accuracy: 0.6000\n",
            "Epoch 106/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4332 - accuracy: 0.8477\n",
            "Epoch 00106: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.4332 - accuracy: 0.8477 - val_loss: 0.8087 - val_accuracy: 0.5875\n",
            "Epoch 107/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.7079\n",
            "Epoch 00107: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.5286 - accuracy: 0.7079 - val_loss: 0.7414 - val_accuracy: 0.4375\n",
            "Epoch 108/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.8046\n",
            "Epoch 00108: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.4677 - accuracy: 0.8046 - val_loss: 0.7745 - val_accuracy: 0.5833\n",
            "Epoch 109/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.7634\n",
            "Epoch 00109: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.4843 - accuracy: 0.7634 - val_loss: 0.8292 - val_accuracy: 0.4250\n",
            "Epoch 110/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.7694\n",
            "Epoch 00110: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.4961 - accuracy: 0.7694 - val_loss: 0.7283 - val_accuracy: 0.6042\n",
            "Epoch 111/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4483 - accuracy: 0.8153\n",
            "Epoch 00111: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.4483 - accuracy: 0.8153 - val_loss: 0.7414 - val_accuracy: 0.4833\n",
            "Epoch 112/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.7565\n",
            "Epoch 00112: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 611ms/step - loss: 0.4854 - accuracy: 0.7565 - val_loss: 0.8178 - val_accuracy: 0.5917\n",
            "Epoch 113/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5113 - accuracy: 0.7255\n",
            "Epoch 00113: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.5113 - accuracy: 0.7255 - val_loss: 0.6993 - val_accuracy: 0.5792\n",
            "Epoch 114/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.8690\n",
            "Epoch 00114: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.4215 - accuracy: 0.8690 - val_loss: 0.7187 - val_accuracy: 0.5708\n",
            "Epoch 115/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.8264\n",
            "Epoch 00115: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 614ms/step - loss: 0.4347 - accuracy: 0.8264 - val_loss: 0.8212 - val_accuracy: 0.5792\n",
            "Epoch 116/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.7620\n",
            "Epoch 00116: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.4802 - accuracy: 0.7620 - val_loss: 0.8700 - val_accuracy: 0.4250\n",
            "Epoch 117/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.7718\n",
            "Epoch 00117: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.4839 - accuracy: 0.7718 - val_loss: 0.7456 - val_accuracy: 0.4833\n",
            "Epoch 118/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.7454\n",
            "Epoch 00118: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.4920 - accuracy: 0.7454 - val_loss: 0.8206 - val_accuracy: 0.5917\n",
            "Epoch 119/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.7921\n",
            "Epoch 00119: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.4521 - accuracy: 0.7921 - val_loss: 0.7365 - val_accuracy: 0.4833\n",
            "Epoch 120/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.8102\n",
            "Epoch 00120: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.4438 - accuracy: 0.8102 - val_loss: 0.7459 - val_accuracy: 0.6083\n",
            "Epoch 121/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8162\n",
            "Epoch 00121: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.4284 - accuracy: 0.8162 - val_loss: 0.8660 - val_accuracy: 0.4250\n",
            "Epoch 122/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.7361\n",
            "Epoch 00122: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.5151 - accuracy: 0.7361 - val_loss: 0.7276 - val_accuracy: 0.5958\n",
            "Epoch 123/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.8731\n",
            "Epoch 00123: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.3884 - accuracy: 0.8731 - val_loss: 0.8138 - val_accuracy: 0.6042\n",
            "Epoch 124/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.7514\n",
            "Epoch 00124: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.4686 - accuracy: 0.7514 - val_loss: 0.7212 - val_accuracy: 0.5625\n",
            "Epoch 125/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8773\n",
            "Epoch 00125: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.3901 - accuracy: 0.8773 - val_loss: 0.9331 - val_accuracy: 0.5917\n",
            "Epoch 126/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.7111\n",
            "Epoch 00126: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 614ms/step - loss: 0.5188 - accuracy: 0.7111 - val_loss: 0.7298 - val_accuracy: 0.5958\n",
            "Epoch 127/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8912\n",
            "Epoch 00127: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 640ms/step - loss: 0.3668 - accuracy: 0.8912 - val_loss: 0.7869 - val_accuracy: 0.4708\n",
            "Epoch 128/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.7269\n",
            "Epoch 00128: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.4998 - accuracy: 0.7269 - val_loss: 0.7457 - val_accuracy: 0.6042\n",
            "Epoch 129/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8861\n",
            "Epoch 00129: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.3703 - accuracy: 0.8861 - val_loss: 0.7866 - val_accuracy: 0.4625\n",
            "Epoch 130/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.8208\n",
            "Epoch 00130: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.4106 - accuracy: 0.8208 - val_loss: 0.9370 - val_accuracy: 0.5875\n",
            "Epoch 131/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.7921\n",
            "Epoch 00131: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.4479 - accuracy: 0.7921 - val_loss: 0.7465 - val_accuracy: 0.5625\n",
            "Epoch 132/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8606\n",
            "Epoch 00132: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 614ms/step - loss: 0.3683 - accuracy: 0.8606 - val_loss: 0.9969 - val_accuracy: 0.5917\n",
            "Epoch 133/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4591 - accuracy: 0.7778\n",
            "Epoch 00133: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 613ms/step - loss: 0.4591 - accuracy: 0.7778 - val_loss: 0.7484 - val_accuracy: 0.5875\n",
            "Epoch 134/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8949\n",
            "Epoch 00134: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 648ms/step - loss: 0.3514 - accuracy: 0.8949 - val_loss: 0.7702 - val_accuracy: 0.6000\n",
            "Epoch 135/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.8620\n",
            "Epoch 00135: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 612ms/step - loss: 0.3722 - accuracy: 0.8620 - val_loss: 0.9455 - val_accuracy: 0.4292\n",
            "Epoch 136/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4502 - accuracy: 0.7843\n",
            "Epoch 00136: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.4502 - accuracy: 0.7843 - val_loss: 1.0176 - val_accuracy: 0.5875\n",
            "Epoch 137/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4604 - accuracy: 0.7648\n",
            "Epoch 00137: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.4604 - accuracy: 0.7648 - val_loss: 0.7864 - val_accuracy: 0.4750\n",
            "Epoch 138/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8051\n",
            "Epoch 00138: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.4210 - accuracy: 0.8051 - val_loss: 0.8306 - val_accuracy: 0.6000\n",
            "Epoch 139/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.8181\n",
            "Epoch 00139: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.4109 - accuracy: 0.8181 - val_loss: 0.8140 - val_accuracy: 0.4542\n",
            "Epoch 140/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8468\n",
            "Epoch 00140: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.3949 - accuracy: 0.8468 - val_loss: 0.8907 - val_accuracy: 0.5917\n",
            "Epoch 141/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.7750\n",
            "Epoch 00141: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.4398 - accuracy: 0.7750 - val_loss: 0.7563 - val_accuracy: 0.5708\n",
            "Epoch 142/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.9125\n",
            "Epoch 00142: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.3272 - accuracy: 0.9125 - val_loss: 0.7855 - val_accuracy: 0.5125\n",
            "Epoch 143/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.7861\n",
            "Epoch 00143: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.4272 - accuracy: 0.7861 - val_loss: 0.7768 - val_accuracy: 0.5958\n",
            "Epoch 144/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3286 - accuracy: 0.9102\n",
            "Epoch 00144: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.3286 - accuracy: 0.9102 - val_loss: 0.8016 - val_accuracy: 0.5875\n",
            "Epoch 145/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8917\n",
            "Epoch 00145: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.3296 - accuracy: 0.8917 - val_loss: 0.8183 - val_accuracy: 0.4708\n",
            "Epoch 146/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8162\n",
            "Epoch 00146: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.4004 - accuracy: 0.8162 - val_loss: 0.8567 - val_accuracy: 0.5958\n",
            "Epoch 147/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8667\n",
            "Epoch 00147: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.3639 - accuracy: 0.8667 - val_loss: 0.9081 - val_accuracy: 0.4500\n",
            "Epoch 148/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4253 - accuracy: 0.8074\n",
            "Epoch 00148: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.4253 - accuracy: 0.8074 - val_loss: 0.9005 - val_accuracy: 0.6000\n",
            "Epoch 149/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3987 - accuracy: 0.8157\n",
            "Epoch 00149: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.3987 - accuracy: 0.8157 - val_loss: 0.8116 - val_accuracy: 0.4708\n",
            "Epoch 150/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.7921\n",
            "Epoch 00150: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.4204 - accuracy: 0.7921 - val_loss: 0.8026 - val_accuracy: 0.5917\n",
            "Epoch 151/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9083\n",
            "Epoch 00151: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.3078 - accuracy: 0.9083 - val_loss: 0.7954 - val_accuracy: 0.5625\n",
            "Epoch 152/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8708\n",
            "Epoch 00152: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.3303 - accuracy: 0.8708 - val_loss: 1.0258 - val_accuracy: 0.5875\n",
            "Epoch 153/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.7972\n",
            "Epoch 00153: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.4208 - accuracy: 0.7972 - val_loss: 0.7964 - val_accuracy: 0.5583\n",
            "Epoch 154/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8546\n",
            "Epoch 00154: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.3508 - accuracy: 0.8546 - val_loss: 0.9841 - val_accuracy: 0.5875\n",
            "Epoch 155/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.8741\n",
            "Epoch 00155: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.3697 - accuracy: 0.8741 - val_loss: 0.7950 - val_accuracy: 0.5542\n",
            "Epoch 156/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.8847\n",
            "Epoch 00156: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.3222 - accuracy: 0.8847 - val_loss: 1.1829 - val_accuracy: 0.5917\n",
            "Epoch 157/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.8537\n",
            "Epoch 00157: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.3995 - accuracy: 0.8537 - val_loss: 0.8082 - val_accuracy: 0.5708\n",
            "Epoch 158/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3001 - accuracy: 0.9014\n",
            "Epoch 00158: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.3001 - accuracy: 0.9014 - val_loss: 1.0460 - val_accuracy: 0.5875\n",
            "Epoch 159/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8597\n",
            "Epoch 00159: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.3491 - accuracy: 0.8597 - val_loss: 0.8050 - val_accuracy: 0.5583\n",
            "Epoch 160/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9301\n",
            "Epoch 00160: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 594ms/step - loss: 0.2723 - accuracy: 0.9301 - val_loss: 0.8570 - val_accuracy: 0.4917\n",
            "Epoch 161/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.8519\n",
            "Epoch 00161: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.3354 - accuracy: 0.8519 - val_loss: 0.9635 - val_accuracy: 0.6042\n",
            "Epoch 162/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8407\n",
            "Epoch 00162: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.3648 - accuracy: 0.8407 - val_loss: 0.8245 - val_accuracy: 0.5167\n",
            "Epoch 163/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8856\n",
            "Epoch 00163: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.3053 - accuracy: 0.8856 - val_loss: 0.9286 - val_accuracy: 0.6125\n",
            "Epoch 164/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8315\n",
            "Epoch 00164: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 619ms/step - loss: 0.3667 - accuracy: 0.8315 - val_loss: 0.7905 - val_accuracy: 0.5500\n",
            "Epoch 165/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9319\n",
            "Epoch 00165: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.2652 - accuracy: 0.9319 - val_loss: 1.0874 - val_accuracy: 0.5917\n",
            "Epoch 166/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.7792\n",
            "Epoch 00166: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.4131 - accuracy: 0.7792 - val_loss: 0.8044 - val_accuracy: 0.5458\n",
            "Epoch 167/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9255\n",
            "Epoch 00167: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.2605 - accuracy: 0.9255 - val_loss: 0.9659 - val_accuracy: 0.6042\n",
            "Epoch 168/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.8148\n",
            "Epoch 00168: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.3721 - accuracy: 0.8148 - val_loss: 0.8459 - val_accuracy: 0.4625\n",
            "Epoch 169/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8764\n",
            "Epoch 00169: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.3270 - accuracy: 0.8764 - val_loss: 0.8714 - val_accuracy: 0.5917\n",
            "Epoch 170/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.8644\n",
            "Epoch 00170: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.3316 - accuracy: 0.8644 - val_loss: 0.8171 - val_accuracy: 0.4500\n",
            "Epoch 171/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.9097\n",
            "Epoch 00171: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.3059 - accuracy: 0.9097 - val_loss: 0.9288 - val_accuracy: 0.6042\n",
            "Epoch 172/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8806\n",
            "Epoch 00172: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.2964 - accuracy: 0.8806 - val_loss: 0.8904 - val_accuracy: 0.4417\n",
            "Epoch 173/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9079\n",
            "Epoch 00173: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.2985 - accuracy: 0.9079 - val_loss: 0.8871 - val_accuracy: 0.5625\n",
            "Epoch 174/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9454\n",
            "Epoch 00174: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.2268 - accuracy: 0.9454 - val_loss: 0.9295 - val_accuracy: 0.4833\n",
            "Epoch 175/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.7968\n",
            "Epoch 00175: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.3798 - accuracy: 0.7968 - val_loss: 0.9075 - val_accuracy: 0.6042\n",
            "Epoch 176/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9481\n",
            "Epoch 00176: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.2285 - accuracy: 0.9481 - val_loss: 1.0028 - val_accuracy: 0.4375\n",
            "Epoch 177/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8380\n",
            "Epoch 00177: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.3629 - accuracy: 0.8380 - val_loss: 0.8639 - val_accuracy: 0.5458\n",
            "Epoch 178/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9644\n",
            "Epoch 00178: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.2048 - accuracy: 0.9644 - val_loss: 0.8876 - val_accuracy: 0.5708\n",
            "Epoch 179/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.8912\n",
            "Epoch 00179: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 615ms/step - loss: 0.2784 - accuracy: 0.8912 - val_loss: 0.9416 - val_accuracy: 0.6042\n",
            "Epoch 180/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9421\n",
            "Epoch 00180: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.2321 - accuracy: 0.9421 - val_loss: 0.9625 - val_accuracy: 0.4667\n",
            "Epoch 181/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.7917\n",
            "Epoch 00181: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.4067 - accuracy: 0.7917 - val_loss: 0.8913 - val_accuracy: 0.5792\n",
            "Epoch 182/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9486\n",
            "Epoch 00182: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.2202 - accuracy: 0.9486 - val_loss: 1.0618 - val_accuracy: 0.4125\n",
            "Epoch 183/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8329\n",
            "Epoch 00183: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.3577 - accuracy: 0.8329 - val_loss: 1.0396 - val_accuracy: 0.6083\n",
            "Epoch 184/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.8949\n",
            "Epoch 00184: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 614ms/step - loss: 0.2763 - accuracy: 0.8949 - val_loss: 0.9220 - val_accuracy: 0.4750\n",
            "Epoch 185/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9231\n",
            "Epoch 00185: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.2425 - accuracy: 0.9231 - val_loss: 1.0332 - val_accuracy: 0.6042\n",
            "Epoch 186/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.8903\n",
            "Epoch 00186: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.2627 - accuracy: 0.8903 - val_loss: 0.8845 - val_accuracy: 0.5167\n",
            "Epoch 187/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9491\n",
            "Epoch 00187: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.2198 - accuracy: 0.9491 - val_loss: 1.0596 - val_accuracy: 0.6083\n",
            "Epoch 188/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.8963\n",
            "Epoch 00188: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.2610 - accuracy: 0.8963 - val_loss: 0.9658 - val_accuracy: 0.4500\n",
            "Epoch 189/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.8921\n",
            "Epoch 00189: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.2999 - accuracy: 0.8921 - val_loss: 0.9960 - val_accuracy: 0.6083\n",
            "Epoch 190/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9264\n",
            "Epoch 00190: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.2220 - accuracy: 0.9264 - val_loss: 0.9223 - val_accuracy: 0.5042\n",
            "Epoch 191/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9472\n",
            "Epoch 00191: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.2091 - accuracy: 0.9472 - val_loss: 1.0359 - val_accuracy: 0.6083\n",
            "Epoch 192/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9324\n",
            "Epoch 00192: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.2176 - accuracy: 0.9324 - val_loss: 0.9426 - val_accuracy: 0.4583\n",
            "Epoch 193/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8806\n",
            "Epoch 00193: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.2940 - accuracy: 0.8806 - val_loss: 0.9687 - val_accuracy: 0.5625\n",
            "Epoch 194/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9639\n",
            "Epoch 00194: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.1780 - accuracy: 0.9639 - val_loss: 0.9844 - val_accuracy: 0.4958\n",
            "Epoch 195/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.8551\n",
            "Epoch 00195: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.2994 - accuracy: 0.8551 - val_loss: 1.0289 - val_accuracy: 0.6000\n",
            "Epoch 196/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9486\n",
            "Epoch 00196: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.2162 - accuracy: 0.9486 - val_loss: 0.9492 - val_accuracy: 0.5542\n",
            "Epoch 197/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9301\n",
            "Epoch 00197: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.2110 - accuracy: 0.9301 - val_loss: 1.0546 - val_accuracy: 0.6042\n",
            "Epoch 198/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9282\n",
            "Epoch 00198: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 593ms/step - loss: 0.2150 - accuracy: 0.9282 - val_loss: 1.0780 - val_accuracy: 0.4125\n",
            "Epoch 199/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.8968\n",
            "Epoch 00199: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.2689 - accuracy: 0.8968 - val_loss: 0.9967 - val_accuracy: 0.5500\n",
            "Epoch 200/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9764\n",
            "Epoch 00200: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 613ms/step - loss: 0.1449 - accuracy: 0.9764 - val_loss: 1.0140 - val_accuracy: 0.5375\n",
            "Epoch 201/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9755\n",
            "Epoch 00201: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.1464 - accuracy: 0.9755 - val_loss: 0.9803 - val_accuracy: 0.5500\n",
            "Epoch 202/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9806\n",
            "Epoch 00202: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 616ms/step - loss: 0.1425 - accuracy: 0.9806 - val_loss: 1.0766 - val_accuracy: 0.5625\n",
            "Epoch 203/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9324\n",
            "Epoch 00203: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.1984 - accuracy: 0.9324 - val_loss: 1.3781 - val_accuracy: 0.4042\n",
            "Epoch 204/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.7838\n",
            "Epoch 00204: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.4699 - accuracy: 0.7838 - val_loss: 0.8397 - val_accuracy: 0.5625\n",
            "Epoch 205/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9727\n",
            "Epoch 00205: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.2165 - accuracy: 0.9727 - val_loss: 0.8988 - val_accuracy: 0.5583\n",
            "Epoch 206/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9565\n",
            "Epoch 00206: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 634ms/step - loss: 0.1974 - accuracy: 0.9565 - val_loss: 1.2342 - val_accuracy: 0.6042\n",
            "Epoch 207/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.8343\n",
            "Epoch 00207: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.3294 - accuracy: 0.8343 - val_loss: 0.9028 - val_accuracy: 0.5292\n",
            "Epoch 208/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9481\n",
            "Epoch 00208: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.2079 - accuracy: 0.9481 - val_loss: 1.2598 - val_accuracy: 0.5875\n",
            "Epoch 209/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9093\n",
            "Epoch 00209: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.2500 - accuracy: 0.9093 - val_loss: 1.0007 - val_accuracy: 0.5375\n",
            "Epoch 210/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9792\n",
            "Epoch 00210: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.1420 - accuracy: 0.9792 - val_loss: 1.1144 - val_accuracy: 0.5750\n",
            "Epoch 211/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9491\n",
            "Epoch 00211: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.1675 - accuracy: 0.9491 - val_loss: 1.1096 - val_accuracy: 0.4375\n",
            "Epoch 212/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9079\n",
            "Epoch 00212: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.2384 - accuracy: 0.9079 - val_loss: 1.0655 - val_accuracy: 0.5250\n",
            "Epoch 213/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9843\n",
            "Epoch 00213: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 648ms/step - loss: 0.1183 - accuracy: 0.9843 - val_loss: 1.1495 - val_accuracy: 0.5750\n",
            "Epoch 214/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9810\n",
            "Epoch 00214: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.1237 - accuracy: 0.9810 - val_loss: 1.1000 - val_accuracy: 0.5458\n",
            "Epoch 215/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9875\n",
            "Epoch 00215: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.1056 - accuracy: 0.9875 - val_loss: 1.1370 - val_accuracy: 0.5458\n",
            "Epoch 216/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9787\n",
            "Epoch 00216: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.1151 - accuracy: 0.9787 - val_loss: 1.0857 - val_accuracy: 0.5542\n",
            "Epoch 217/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9509\n",
            "Epoch 00217: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.1657 - accuracy: 0.9509 - val_loss: 1.3971 - val_accuracy: 0.5917\n",
            "Epoch 218/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8806\n",
            "Epoch 00218: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 612ms/step - loss: 0.2700 - accuracy: 0.8806 - val_loss: 1.0476 - val_accuracy: 0.5375\n",
            "Epoch 219/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9884\n",
            "Epoch 00219: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.1087 - accuracy: 0.9884 - val_loss: 1.1374 - val_accuracy: 0.5375\n",
            "Epoch 220/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9898\n",
            "Epoch 00220: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.0998 - accuracy: 0.9898 - val_loss: 1.2123 - val_accuracy: 0.5708\n",
            "Epoch 221/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.9565\n",
            "Epoch 00221: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.1523 - accuracy: 0.9565 - val_loss: 1.0380 - val_accuracy: 0.5375\n",
            "Epoch 222/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9856\n",
            "Epoch 00222: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.1141 - accuracy: 0.9856 - val_loss: 1.1831 - val_accuracy: 0.5833\n",
            "Epoch 223/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9829\n",
            "Epoch 00223: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.1131 - accuracy: 0.9829 - val_loss: 1.1193 - val_accuracy: 0.5042\n",
            "Epoch 224/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.8981\n",
            "Epoch 00224: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.2277 - accuracy: 0.8981 - val_loss: 1.0529 - val_accuracy: 0.5958\n",
            "Epoch 225/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9727\n",
            "Epoch 00225: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 596ms/step - loss: 0.1707 - accuracy: 0.9727 - val_loss: 1.0623 - val_accuracy: 0.5125\n",
            "Epoch 226/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9236\n",
            "Epoch 00226: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 601ms/step - loss: 0.2055 - accuracy: 0.9236 - val_loss: 1.2463 - val_accuracy: 0.5833\n",
            "Epoch 227/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1312 - accuracy: 0.9792\n",
            "Epoch 00227: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.1312 - accuracy: 0.9792 - val_loss: 1.1252 - val_accuracy: 0.5500\n",
            "Epoch 228/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9917\n",
            "Epoch 00228: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.0912 - accuracy: 0.9917 - val_loss: 1.1973 - val_accuracy: 0.5333\n",
            "Epoch 229/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9935\n",
            "Epoch 00229: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.0822 - accuracy: 0.9935 - val_loss: 1.2431 - val_accuracy: 0.5500\n",
            "Epoch 230/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9944\n",
            "Epoch 00230: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.0846 - accuracy: 0.9944 - val_loss: 1.1791 - val_accuracy: 0.5375\n",
            "Epoch 231/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9944\n",
            "Epoch 00231: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.0769 - accuracy: 0.9944 - val_loss: 1.1820 - val_accuracy: 0.5417\n",
            "Epoch 232/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9949\n",
            "Epoch 00232: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 597ms/step - loss: 0.0822 - accuracy: 0.9949 - val_loss: 1.2005 - val_accuracy: 0.5375\n",
            "Epoch 233/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9829\n",
            "Epoch 00233: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.0945 - accuracy: 0.9829 - val_loss: 1.2203 - val_accuracy: 0.5375\n",
            "Epoch 234/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9963\n",
            "Epoch 00234: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.0766 - accuracy: 0.9963 - val_loss: 1.2706 - val_accuracy: 0.5333\n",
            "Epoch 235/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9921\n",
            "Epoch 00235: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.0779 - accuracy: 0.9921 - val_loss: 1.2213 - val_accuracy: 0.5458\n",
            "Epoch 236/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9931\n",
            "Epoch 00236: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.0759 - accuracy: 0.9931 - val_loss: 1.2743 - val_accuracy: 0.5333\n",
            "Epoch 237/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9931\n",
            "Epoch 00237: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.0706 - accuracy: 0.9931 - val_loss: 1.2518 - val_accuracy: 0.5417\n",
            "Epoch 238/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9935\n",
            "Epoch 00238: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 614ms/step - loss: 0.0708 - accuracy: 0.9935 - val_loss: 1.2979 - val_accuracy: 0.5375\n",
            "Epoch 239/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9972\n",
            "Epoch 00239: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.0662 - accuracy: 0.9972 - val_loss: 1.3246 - val_accuracy: 0.5375\n",
            "Epoch 240/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9926\n",
            "Epoch 00240: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.0671 - accuracy: 0.9926 - val_loss: 1.2652 - val_accuracy: 0.5458\n",
            "Epoch 241/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9972\n",
            "Epoch 00241: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.0650 - accuracy: 0.9972 - val_loss: 1.3568 - val_accuracy: 0.5500\n",
            "Epoch 242/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9931\n",
            "Epoch 00242: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.0711 - accuracy: 0.9931 - val_loss: 1.2662 - val_accuracy: 0.5417\n",
            "Epoch 243/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9750\n",
            "Epoch 00243: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.1014 - accuracy: 0.9750 - val_loss: 2.0376 - val_accuracy: 0.5833\n",
            "Epoch 244/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.6986\n",
            "Epoch 00244: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 603ms/step - loss: 0.6049 - accuracy: 0.6986 - val_loss: 0.8037 - val_accuracy: 0.5000\n",
            "Epoch 245/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.9014\n",
            "Epoch 00245: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.2930 - accuracy: 0.9014 - val_loss: 1.0367 - val_accuracy: 0.5917\n",
            "Epoch 246/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.8921\n",
            "Epoch 00246: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.2947 - accuracy: 0.8921 - val_loss: 0.9796 - val_accuracy: 0.4625\n",
            "Epoch 247/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.7755\n",
            "Epoch 00247: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.3998 - accuracy: 0.7755 - val_loss: 0.9173 - val_accuracy: 0.5917\n",
            "Epoch 248/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9338\n",
            "Epoch 00248: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.2455 - accuracy: 0.9338 - val_loss: 0.9359 - val_accuracy: 0.5542\n",
            "Epoch 249/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9722\n",
            "Epoch 00249: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 602ms/step - loss: 0.1595 - accuracy: 0.9722 - val_loss: 1.1949 - val_accuracy: 0.6083\n",
            "Epoch 250/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.8069\n",
            "Epoch 00250: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.3504 - accuracy: 0.8069 - val_loss: 0.8560 - val_accuracy: 0.5583\n",
            "Epoch 251/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9681\n",
            "Epoch 00251: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.1900 - accuracy: 0.9681 - val_loss: 1.1053 - val_accuracy: 0.6167\n",
            "Epoch 252/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9347\n",
            "Epoch 00252: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.2095 - accuracy: 0.9347 - val_loss: 0.9705 - val_accuracy: 0.4583\n",
            "Epoch 253/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9551\n",
            "Epoch 00253: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 609ms/step - loss: 0.1916 - accuracy: 0.9551 - val_loss: 1.1457 - val_accuracy: 0.6000\n",
            "Epoch 254/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9606\n",
            "Epoch 00254: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.1651 - accuracy: 0.9606 - val_loss: 1.1133 - val_accuracy: 0.4708\n",
            "Epoch 255/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8560\n",
            "Epoch 00255: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.2927 - accuracy: 0.8560 - val_loss: 1.1327 - val_accuracy: 0.6042\n",
            "Epoch 256/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9718\n",
            "Epoch 00256: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 610ms/step - loss: 0.1464 - accuracy: 0.9718 - val_loss: 1.0614 - val_accuracy: 0.5583\n",
            "Epoch 257/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9778\n",
            "Epoch 00257: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 611ms/step - loss: 0.1288 - accuracy: 0.9778 - val_loss: 1.1509 - val_accuracy: 0.5583\n",
            "Epoch 258/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9931\n",
            "Epoch 00258: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 607ms/step - loss: 0.0942 - accuracy: 0.9931 - val_loss: 1.0959 - val_accuracy: 0.5542\n",
            "Epoch 259/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9931\n",
            "Epoch 00259: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.0940 - accuracy: 0.9931 - val_loss: 1.3521 - val_accuracy: 0.6125\n",
            "Epoch 260/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9111\n",
            "Epoch 00260: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 605ms/step - loss: 0.2119 - accuracy: 0.9111 - val_loss: 1.0482 - val_accuracy: 0.5458\n",
            "Epoch 261/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9963\n",
            "Epoch 00261: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.0848 - accuracy: 0.9963 - val_loss: 1.2444 - val_accuracy: 0.5417\n",
            "Epoch 262/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9935\n",
            "Epoch 00262: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.0776 - accuracy: 0.9935 - val_loss: 1.1852 - val_accuracy: 0.5417\n",
            "Epoch 263/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9917\n",
            "Epoch 00263: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 595ms/step - loss: 0.0820 - accuracy: 0.9917 - val_loss: 1.2465 - val_accuracy: 0.5458\n",
            "Epoch 264/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9954\n",
            "Epoch 00264: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 594ms/step - loss: 0.0759 - accuracy: 0.9954 - val_loss: 1.2721 - val_accuracy: 0.5500\n",
            "Epoch 265/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9935\n",
            "Epoch 00265: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 606ms/step - loss: 0.0767 - accuracy: 0.9935 - val_loss: 1.2355 - val_accuracy: 0.4875\n",
            "Epoch 266/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9171\n",
            "Epoch 00266: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 608ms/step - loss: 0.2003 - accuracy: 0.9171 - val_loss: 1.3468 - val_accuracy: 0.6125\n",
            "Epoch 267/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9741\n",
            "Epoch 00267: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 617ms/step - loss: 0.1309 - accuracy: 0.9741 - val_loss: 1.2580 - val_accuracy: 0.5417\n",
            "Epoch 268/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9986\n",
            "Epoch 00268: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 598ms/step - loss: 0.0600 - accuracy: 0.9986 - val_loss: 1.3419 - val_accuracy: 0.5458\n",
            "Epoch 269/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9963\n",
            "Epoch 00269: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 600ms/step - loss: 0.0637 - accuracy: 0.9963 - val_loss: 1.3501 - val_accuracy: 0.5375\n",
            "Epoch 270/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9963\n",
            "Epoch 00270: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 599ms/step - loss: 0.0596 - accuracy: 0.9963 - val_loss: 1.3229 - val_accuracy: 0.5417\n",
            "Epoch 271/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9949\n",
            "Epoch 00271: val_accuracy did not improve from 0.62083\n",
            "5/5 [==============================] - 3s 590ms/step - loss: 0.0638 - accuracy: 0.9949 - val_loss: 1.3454 - val_accuracy: 0.5375\n",
            "Epoch 00271: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['accuracy'],label='train')\n",
        "plt.plot(hist.history['val_accuracy'],label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "92UFwdoMOF8K",
        "outputId": "567eebce-6f16-4a38-855b-b5dff2a3fb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5gc1ZW339vVeXJQzhJCgQxCCGMw0chhscE2Bsy3Zheb9drGmF17F9as13m93jXOrI29OAMmGIMNNjkHC0lIoJylmZE0OXeuru+PW7e6qsPMSJrRBN33efRouqqm+nZP969O/e455wrLstBoNBrN+Mc32gPQaDQazfCgBV2j0WgmCFrQNRqNZoKgBV2j0WgmCFrQNRqNZoLgH60nrq+vt+bOnTtaT6/RaDTjkjVr1rRZljWp2L5RE/S5c+eyevXq0Xp6jUajGZcIIfaW2qctF41Go5kgaEHXaDSaCYIWdI1Go5kgjJqHXox0Ok1jYyOJRGK0hzKihMNhZs6cSSAQGO2haDSaCcSYEvTGxkYqKiqYO3cuQojRHs6IYFkW7e3tNDY2Mm/evNEejkajmUAMarkIIe4SQrQIITaU2C+EEN8XQuwQQrwphDj9cAeTSCSoq6ubsGIOIISgrq5uwt+FaDSao89QPPRfACsH2P8uYKH97wbgf49kQBNZzBXHwmvUaDRHn0EF3bKsF4COAQ55H/ArS/IaUC2EmDZcA9RoNJojIZuVLcJHq1V4a2+SB9Y0kspkR/y5hiPLZQbQ4HrcaG8rQAhxgxBitRBidWtr6zA89fDS1dXFHXfccci/9+53v5uurq4RGJFGoymGZVkc7E6wp62f1Xs6SGZM/u+l3fz6NW/NzQvbWjnja0/y61f3cMl3XuCWB98cdmHf09bPzb9bR0NHDIBnt7awq7XP2X/Xy7v53P3reed3nuejd63it3/dS38yM6xjUBzVSVHLsu4E7gRYtmzZmFtZQwn6Jz/5Sc/2TCaD31/6rXrsscdGemgajQZIpG3hfnUvB3ty81Cfv3QRP35uJ73JDFsO9NAVT5NImbyys51ExuTfH94IwI6WPhZPreC6c44sIeHhdU08samZ5XNrCRg+HnqjiYfeaOLq5bO4Z1UDhk/w3x88mStOn8k+W+inV0do6IzxhYc20J/McMN5C45oDMUYDkFvAma5Hs+0t407brnlFnbu3Mmpp55KIBAgHA5TU1PDli1b2LZtG+9///tpaGggkUhw0003ccMNNwC5NgZ9fX28613v4u1vfzuvvPIKM2bM4OGHHyYSiYzyK9NoxjaJtMmjbx4g4Pdx2SnTSx737Se28tMXd3P+okl88oIFRIN+fvL8Tu56aTe9yQxVkQB3r9rHnNoo4YDBxUun8PFz5/HZ363jo2fP5eF1Tdy/pvGIBf32J7extz3G67s7+MfzpTDPrYtyz6oGLl4ymY37e3hqc7MU9PYY5y6s59fXn4VlWazd18X8+rIjev5SDIegPwJ8WghxL3AW0G1Z1oEjPemX/7iRTft7jnhwbpZOr+Q//uaEkvu/+c1vsmHDBtatW8dzzz3He97zHjZs2OCkF951113U1tYSj8c588wz+cAHPkBdXZ3nHNu3b+eee+7hpz/9KVdeeSUPPvgg11577bC+Do1mPHOgO84tD77F195/IrNqowDc+vu3eOgNGQdedsp0OvtT1JQFC373qc0tXLBoEj//u+XOtj1t/fzw2R0APPqZt1MRDlAV8dZ4PPPP5wOwvzvOz1/aQyqTJeg/PMe5rS/J3nYZdcfTJvG0CcCfbzqPLQd7OGF6FR/68SvEUiaWZbGnvZ/3z5IutBCCM+bUHNbzDoWhpC3eA7wKLBJCNAohrhdCfEII8Qn7kMeAXcAO4KfAJ0ucatyxfPlyT67497//fU455RRWrFhBQ0MD27dvL/idefPmceqppwJwxhlnsGfPnqM1XI1mTPNmYxe3PPgmX390M89va+WXr+xx9h3sztknj288yGlffZIvPrzBmdAEaOiIsbutn3MXehsNvmORfLxgUhkza6IFYu5m6bRKUmaWnS6P+1B5Y5+cL1s+t5Z4yiSRMhECwgEfp82uIej3EQkaxFImXbE0vYkMc+qih/18h8KgEbplWVcPst8CPjVsI7IZKJI+WpSV5W6LnnvuOZ566ileffVVotEo559/ftFc8lAo5PxsGAbxePyojFWjOdo8u6WFmTURFk6pGNLxj755gHtfl/kTQdt3/peViwn6fSQzpnPcc1tbAPjVq3tZeeJU3ragHoCXdrQBcN7x9Z7znjarmvryIOcvmjzoGE6YXgnApv09LJlWOaRx57N2Xyd+n2D5vFpW7emgJ5EhEjA86cjRoJ+W3gR7bf98Tt3IWCz56F4uLioqKujt7S26r7u7m5qaGqLRKFu2bOG11147yqPTaMYGibTJG/s6+btfvM7N960b8Njrf/E633lyGwAq1j5/0SS+ccVJtPenHPFOZrJEgwaQi4ABOvpTzs+v7mxnSmWIBZPKPc/hN3z8+abz+PyliwYd+7z6csIBH5sOHL6du3ZvJydMr6Q6GnDGGAkYnmOidoS+t70fYOxE6McSdXV1nHPOOZx44olEIhGmTJni7Fu5ciU//vGPWbJkCYsWLWLFihWjOFKNZnTo7E9x1n8+7eRUB42BY8K3mrpJ2NF3fzJDXVmQX/zdchJpk8/dv57NB3p55wlTSWayzK6NsuVgL1ubex1BjKVykXtLb4I5tWVFC/MmVYQKthXD8AkWTa1k4/7uob5kD5v297BqTwefOv84okEpnx39KcLFBD1pOl777Fot6KPC3XffXXR7KBTiz3/+c9F9yievr69nw4Zch4TPfe5zwz4+jWY0aelNknJF06Yddj+/rZUfPL2d+/7hbHy+nOD2JzMc6JLWZDxlErF/LxwwmFQRoqlLCl4yYzJ/aiVbDvZiWbBgUrm8GKRN/ul363jnCVPoTWSYWhk+4tcwqyZSMuHizhd2snRaFW9fWF90/zce20x1JMDHz5vPM1uaAWjvTzmvSxEN+omlMuzvijOpIlQg+COFtlw0Gs2QiaVkQcwPrzmNi5ZMoTsmLZFbH3yT1Xs72dXW7xybzVrE0ib7u+NYlkUsZVIWzMWQM6ojNHXJOaZkOktdeZDykNy/YJL0nOMpkz++uZ/nt7XRk0hTOcCE51AxfAKzRHHRNx7bwrX/91fa+pLscb0WADNr8dKONq5aPpuqSIBIQEXoyaKWSzxt0h1PUz0MYx4qWtA1mgmEmbU8mSHDTdy2QCIBP9WRAF3xNABLp1cBMpNFkciYWBYk0lm6Ymn6UxlPJDujJkJTpy3omSwhv8HkSmmdzKuXPnlPIk3atOiJp+mJZ6gMH7mpYAhBxix8jxLpnL1z8e3Pc/7/POfZryZulUCr19LZny4SoRukTYv2/hQVwzDmoaIFXaOZQLzn+y9y54u7jvg8n757Lb95rXDpSpVzHQ0aVEcDdMfTZLMWc+1Jv/UNOUHvc5W37++OE0+ZjlUDMLM6wv6uBNmsRTJjEvL7mFIhLZWZNRFCfh9tvfIOoDOWojeRpiI8PBF6tkiE3u6agO2KyQuVu02AM29g56+r15IyswUResS+E2npSQzLmIeKFnSNZgKxtz3G7tb+wQ8chJd3tHmyTRRqkjIaNKiKBLAs6E1kHAtjfWNusjGWzEW8B7sT9KdMZyIRpGinzCytfUk7QvcxxY7Qp1dHiAYN2vuTAOzvipO1oDIyDBG6T2AWuYtp65XPFTBycwCdsTQbmrq5/I6XnRL+kF+Kt1vE8wW9zBb7gz2Joxqh60lRjWYCkTKz9KeOvPFTLGV6csMVjuUSNKiJykrOrnjKEchN+3ucKkxvhJ4gnsp4IvQZNbIlxu62fiwLQgGDKfak5/TqMJGA4UTNymsfjmjXVzJCl4JeFQnQ1ief90B3nPf+4CUgd7FSEbrbZsm3XNTjRDo7LL7/UNERukYzQTCzFmbW8qT6HQ7SAsmSLNLuVU2KRoN+Jw+7KyZ9bpAXlAPdcfvY3DgOdMXtCN0l6NXSptll31GE/D7OmFPDoikVTKuKEA4atNvCqs5fORyWiygVocvnqowEnIj7t3/d5+yP2689lGe5AEXSFnOxsvbQR4nDbZ8L8N3vfpdYLDbMI9Joho7yeI+0NavyyYsKep6HDtAVT2Nmc8cmi4zjQHfC9tBdWS52hK5azYb8Pt55wlQev/k8WT4fMGjvS3qefyQtl9a+XIRea/eReWLjQWd/T1y+HidCH4LlAsNzERoqWtBdaEHXjEcSaZN/e+itopHxuoYurvv5KtKmV5w/d/96vvLHTc5jM2vx5KZmLMvKCXra5O6/7uPm3+WqQeN235KQ30dVxLZcYilP1kg8ZfIfD29gy0FZdV0VCbC/K04sz3IpD/mJBg3HTlHetCISMOjPu9sYFstFCIolAqm7gYDPR02ZfJ62vpQj4D2JtD3OQsslWsJykWPWHvqo4G6fe8kllzB58mTuu+8+kskkl19+OV/+8pfp7+/nyiuvpLGxEdM0+fd//3eam5vZv38/F1xwAfX19Tz77LOj/VI0xwDrG7r45St7uO6cudz9130ssnuqxFwe+pq9nTy3tZXO/hSTXUU56xu6PE2sntnSwsd/tZpf/v1yp7VrMpPl3x56C4BvffBkAoaPWMp0+paoCL07nibtUsg97f388tW9LJ4qxzO3Lsr+bjmpme81l4X8Tnl/KOCNL/OPBYYnbdFHccvFjtDT2SyGqxr1pBlVrNnbSY+doqkEPmj48AmKvi73ncjRjNDHrqD/+RY4+NbwnnPqSfCub5bc7W6f+8QTT/DAAw+watUqLMvisssu44UXXqC1tZXp06fz6KOPArLHS1VVFbfffjvPPvss9fXFK8w0muHmup+vojOW5tITpwK5NMFYyuSJjQdp60s5kXm+fdKb8Noy25plNL16TwfTqsIFv7OrtZ9FUyuIuXxwdUHoinktF3VutYLPzNooG+zKzLJ8QQ8adNrFSaG8drb5NgYM36RoMUFXk6JpM4v7hubE6ZVS0BPKQ5fjEkIQDfrpS2aKlv7nxqw99FHniSee4IknnuC0007j9NNPZ8uWLWzfvp2TTjqJJ598kn/913/lxRdfpKqqarSHqjlGUUU9yjtXlkB/MsPdq/Zx18u7yZQU9LRzPMB2W9DX7O10LJtkxqS+XKYRbjkoBTnuKg4KGD7KQ37PpKg8txQ+ZZfMqok6AuqOXCEvQs+3XIpE6MMhjv4SlaJqUjRjWs77BnDSzGoAJ0J3X3jUGItViubGrCP0ASPpo4FlWdx66638wz/8Q8G+tWvX8thjj3Hbbbdx0UUX8cUvfnEURqg51lGapFIJlZDGUrLkPGNmSansE5egm1mL/pTpEbUd9sTkuoYu+uzzJNNZZlSHaetLsulAD+87dYaM0AM52aiKBOiKp8iYWYTAzkvPXSgChmBqZa5xVoHlEvTTGSsUSigUyZDfNyw9UUpluXgj9Nz+E2fINru9CW+Wi3uMkaB37DrLZQzgbp976aWXctddd9HXJz/oTU1NtLS0sH//fqLRKNdeey2f//znWbt2bcHvajRHQncs7VlkeDCUZ64EJ5O1aO1NknZFmu6ccmXNJNJZkhmTbNZiR0sf06rCxFIma/d12r+TdSLvzQfkZzueNj2iXB0N0B1Lk8laTp8Wt51TFvJTWx5yPc6LZEOGI56DeejDFemq5mHuFglm1nLuFDJZy5Um6WfBpHL8PuHc0bhXOoqWiNDDAR/Khtd56KOEu33uk08+yTXXXMPZZ5/NSSedxAc/+EF6e3t56623WL58Oaeeeipf/vKXue222wC44YYbWLlyJRdccMEovwrNeCGWynDr79+iO5b2bP/hs9v5f/+3asjnUdZGnysybu5JkDKzjofujtDdEXRPPENTV5xEOstlp8q1PNfZ5fvJjOn0N3mrsYsdLX0eDx1k9NmbzJAxLUeA3QVFZUE/da6l5CKBQstFUSzLRZ5D/j8cKYuAM+Fp5pX1K31PZ+T7dsVpM3jqn95BwPAR8vtclkth/nkkz0oSQhC19+ksl1Ekv33uTTfd5Hm8YMECLr300oLfu/HGG7nxxhtHdGyaicWbjd3cs2ofFy2ezMVLc733O/pz/nZ3LE1VdOAILz9CB1mIkzZzEXYyUzhpCdJ332f37F4+t5afPL/LWQ4umcmSSJvMrInQFUvzvh++RHU0yJJpuRWKAoZdEerzURY0aMV7wSgLGU5ONxSm97knSUtZLpWRAD4hhj1CN7MWKrBOuyZ103aBVjRkOJlBoYDhRPBDidBBinwsbVIe1JaLRjPhUUIcS3tzrZMZk7SZ5a+72jnja0961ttUxF352bE8D12RsUUdchH6a7vanXx1kCmHe+xVdU62J/+aexLO78TSJhcsmsztV55Cf8qkqSvuiUaDhk9eOLJZxzfucY0jmhehF1guwQEidFsso0GDykhgWFIWQRYWAZ7y/7Trgpex72wCrsU7wi4RDw1R0MtCBuVBv6c//EijI3SNZpTos5tXxfN6ryTSWTKmxcGehOOHT63yLuzQ5qqgVE2w+vIqRFNm1in4SWaydMfTXPPT11g2t9Y5pieeprknSdDwUV8epCLk93Qd7E1kCAd8TK+OONuiLvEKGD7SGQuB5Yib+8JSHvJT47Zc8qLVcpflEi7hoUeDfs45rpoZrjEcCcpyybg8dPVzOOCTF8KsV9BDrtfsjtDDJSZFQYr80bRbYAwKumVZRZeYmkhYJZrra44tYq68cTfJjEkma5FMF05oKlpdgq6acbnTEAHbcrEjdNOksTNG1oItrvU0u+NpWnoTTKoIIYSgMhKg13VhMLMW4YDh5KaDd7Iy4JcRus8nqAmpSdHcOKJBg4DhozLspyeR8VwMQE6KKkp56NGgwVfed2LBe3C4GEUmRdUdTDTopz+Zse2YnA6pqNwnZNqjM37XCkz55NtLR4MxZbmEw2Ha29sntOBZlkV7ezvh8JEvpaUZ36jJzAJBt4VcWTLFeqqoVq+Qs1/yI3TLwlnPM5nOOotJ9Hg89AwtPUlnTc6qIhkZ4YD0wdX6oW6hChiCdDZLxswSsSNsz6SoLfJ1dqZLNJTvobssl/wI3SXow4nh8tAVKkKPBAzSZpZM1sLvK4zQg36fJ+B00haLCPpxk8tZOKWiYPtIMqYi9JkzZ9LY2Ehra+toD2VECYfDzJw5c7SHoTlEnt/Wyu/XNvK9q07DzFqOMBwuqnlVPE/QlQjH0rkCn3xUe1fIXRCKxUFqX8rMOj1T3PTYEfo8u9y/lKALIZhSFaKhI+4Rr6BtuQR8FgHDR9Dw5aUtymNry4Ls64gVLCrtznLJ3xdWlktoeGXKmRR1e+j2nUwkaDjZLsUi9EKfX44tv2AK4FsfPGX4Bj1ExpSgBwIB5s2bN9rD0GiK8urOdh5et5/r3jaXy+94hXtvWMGK+XUFx8VSGZ7a3MJlp0wf8HzKKsmP0FWqoPLG3SmHX3joLaZXRzzNsGID9D93qj7TWVp6c5Orfp/A5xO2oCc5a558HcVSA5W3PbUyLAXdFTH7DWFPigop6Hl90FUEXlsWJGpfGNyoLJeg4SuYPIzmpS0OF8pDdyW2OILuvfsonAgN5mXiLJhUxvSqcEGGzmgxpFEIIVYKIbYKIXYIIW4psn+OEOJpIcSbQojnhBA6/NRMONSXXuVp37+6sehx33hsM5+55w1W7e4Y8HxKsONpryA77WeLWC7PbW3l2S0ttPblxDm/I6HnOexz5EfoFWE/VZEArb1JumJpJg9kudhR6dQqOSnpjkYDhs+ZfPX7hCN4PgH/dMnxvPdkeVFbOLmcmbXRgnOrCL2YILonRYcTpdPeCD1nuSj8RuHkZ/5dxIeWzeKVWy86qpksAzHoOyWEMIAfAZcAjcDrQohHLMva5Drsf4BfWZb1SyHEhcB/Av9vJAas0YwWStBVPvTOEtWcKl/5YE9huiGo0vuMY7mUitDjruha0RlLYVkWFWG/LOpJZAosGze5CN10PHT1GgKGcEr+1eLMpSwXwCnhd0exKm0xk7XwG8IRvJDf4DMXLXSOu/mS47nxwoXkoyyZfP8cRs5D96ksFzPr5PlnikboRSyXIuMcSwxldMuBHZZl7bIsKwXcC7wv75ilwDP2z88W2a/RjHvyF5AoJeiqXaqqLMznd683cN63nnWaa/Un8wVdReheDz2RNomlTJp7k+xtjzltbvOXnHMLkboLSJpZmroSTk54eUhG6NublaCHPWN3o1LyVIQeybMlVIsBv8+Xay2bF3EHDF/RZlsq+s73piF3ISkbZg9dzX3cs6qBU77yBDtaekmZuSwX95gVanz5EfpYYyijmwE0uB432tvcrAeusH++HKgQQhSYi0KIG4QQq4UQqyf6xKdm4qG+9Moj7k1kPKlvCpV7nJ9GqNjd1kdXLE1jp6zQLLRclIfutVzUSvRm1mJXW78zkemO8A2ft6JSWS498QxtfUlOn1PjjLEyEnAWs3AsF7sqtcKdH64sF1v0831m0+59Yrgsl6F6yuUDWC6V4QA+Ufyu4UhQgr6jRfan+c1r+5w5Cc/8gMtGUfMIoSLZLGOJ4brcfA54hxDiDeAdQBNQcB9oWdadlmUtsyxr2aRJk4bpqTWaI+eHz2xn7i2PDniM8lndWRx7OwpXqVKZD2rJsnyUMDd0FK4wZFmWK0L3CrrqG66YV18OeCdNg4bPI7hKsPfa1aCnzZbVoBXhAAsmlTvHTa6QYq3E091uQInYmXNrOHt+HUumVTr7An7hPE/AbbkM0ZpQY82P6NUY7vn4Cq44PT9+PDKUoKvWwH9cvz+X5VKigEhF6KEJEKE3AbNcj2fa2xwsy9pvWdYVlmWdBnzB3tY1bKPUaI6QrQd7+fz964u2TQX4nye2AYW53G5SduTsXivTXaSjUH5sR3+yYB/k+pjH87xy8E6AOh56CUGfW184yRj0+zy53eoi1GLnrS+cXIFPyC6CN19yvHOc6reiLJeaaK66U0WnkyvD3HPDCkcIQS7XBvKuwW/4PKv5DAVnUrRE5HvW/LrhnxS1PXR1IWzvT7GhSf4do54I3Z2HPnE89NeBhUKIeUKIIHAV8Ij7ACFEvRBCnetW4K7hHaZGc2S8tKON+9c00tGfKlq4pmySYn1TFEoc3aLfkSeykBMKd664m/zuirESgp7voXfl/Z6yXNwE/T6uP3ceVy+f7dmuJmqrIgHOXlDHSTOrKA/5WX3bxfzuhhVO1KpavVa7IvRiRTMKt1/v91guQ7MmQn4fhk8c1bQ/lZGSdC1ioVI681MyFaFDvFCNFoNe+izLygghPg08DhjAXZZlbRRCfAVYbVnWI8D5wH8KISzgBeBTIzhmjeaQUUukNfckOPdbz/Dbj63gDNtPBhmR9iYytPQkOG5yedFzqNtyt+USSxZmmCQdQfdG6E9tasa0rIJIWwn6rb9/y9P7w/HQ094IXS0kMaeuiKAbPq5cNosZ1RHuWbXP2a5+tzzk57cfW+Fsry8PeSJux3Jx+dYDLSoRcAmx3+crma9dCrmMm3FUBT0/QofcXVd+Bo9CvQdjPUIf0r2MZVmPAY/lbfui6+cHgAeGd2gazfChomvV+3tna1+eoAfY11E61RByQt2XzEXK+SmH4IrQXeX52azFbX/YQHU04FguCtWc65ktzd4JzbTXclER+ry6Mtr6klRFAgTtPHCFEsZAXiSpbkoGaxalhNxruQwUobsE3RCuisqhC19Z0D/kiH44UHcjxe6GvHnoEzBC12jGM8mMiWXhZDEoX7o/zyuvtgWsuae47w25CL0vmSFoyBVpYulCz11ZJG19KafZ3JtN3RzsSZDImJ6o3iekcFuWRWd/2tsWN+m1XDr7U0QCBifMqOKAXSQUMATua4qKjN1i5KZ8kBTAqkiA8pCf2a4ioPwuiG7cAuexXA4hG2RqVdjpJXM0cCwXV9ti9XmIDJK2eDQvPIeDFnTNhOarf9rE7rZ+Tp8to/FYCUFXKWrNA0TojqAnMgT9PgKGKGq5pJwOh1l6EhmqIgEe33gQKPTB68pDtPYm6ehPkTKznmhb/RxLmnz0rlXsbO2jJhrg65ef6FygAn4fbkVXIlQqkhwspzvo9/HkP51HbVmQrz+2GcilLRbDG6H7nOc9lEj2/z667KimAzqWizmw5eKeH1AXtaFaSaOFFnTNhOS/H9/CWfPq2N+V4EB3wrFcnNV9ivQOh0EEPZObFA36fUQCRlHLxV3Z2dorrZGnNzfjE5CfZFNvC3qxxlmKfR0xNtnZNEunVXqKf/KtlWAJywWkbTAUQZpmFxA5ZfwDlLX7S02KHoLXXFd+9KJzcFkurr9TnxOhl8hycSL0sS3oY3t0Gk0er+1qH7DUXfHLV/byxKaDpDKyz4iaFC1luagv90AeeirPcokGjaKNsdyRX1tfkkTaZEdLn3OXALnor75cWj37BxB09yRqdd5ydIE8sVWRcaCI5XKoiy2E/D7PSj3FCOZ56IdaWDQaKEH3RuiFHvpQmnONNcb26DQaFy29Ca668zVu/t26QY9NZbLOv4xrbU2V+51fbq9S2FoG8NDVZGcinSXo9xEN+YmlTP7wRhNr93XmzpXOOrfu/ckMO1r6yFpw/qJcMZ3yqCfZ0WlTV+kLSbsr/TE/rTLgH3qEPph/nk/Ibww4IZr/PH6fIGiM/UhWDTmVyTrjHNxyGR8e+th91zWaPJRv/PqegbsYWpYl/ehMlqSZJZ21yGTVohHF199UE2QtvYmi5fyQ89BBCmc0ICP0rz26mV+8vCd3LjPrZIn0p0y2HpQl5u84frJzzHy7SlNNBrobZ+XjjiTzLZ5DsVzKDyNCL9Z/xfv8bsvFd8h56KOBz5W2qOYU+lMZhPCO291tMaQ9dI1meFFVnvlinI9afUaJetq1tma+5fLkpmYiAcMRzbQp88SL+bpuYQ0aPspCBge603THU55l11KZLNXRAE1dcWLJDLvb+gn6fSyZVkFVJEB3PM38STKHXOWAD2S5KK45azbX5BUMKeFW/nxO0Astl0OO0AM+pxK0FJ48dJflMpaFL5e2aHCzdyYAACAASURBVBIJGAj13hk+z/tWfIGLsfu6QEfomnGEO3tkwOMyuZXuUxlTLvqrJkWV5WJ73997ehs/fn6np8ikVPm/e2X4oN9HJOinvS9F2rQ8v5PMmE6EHkuZbDnYy3GTyvEbPmdtzmVzagkYgrl2tedAk6KKT11wHCfOqPJsC9qi4zS5Uh66S3jUmhLloUNrciUtl0Pw0F2FRWNZ+NwRut/wuXL3hefOxv1z2LUE3VhmbI9Oo3GRHkTI849LZmQaYNrMFkyK9tlRfl8iQyyVIWlH1VC8WEieN2fFBA0fZUHDKRl33zWkMllnpftYKsPWg70smirXlpxqr25z8ZLJvP6Fi5lbJ730oUToxSJsJTpqX7FeKuV2bvXhTIoOlk7oyXLx9EMfu9LiLiySvr/K3fd5Xo9b0NX7e6jv4dFmbI9Oo3Gh0gYHw4nk1aRo1iKdVZOitpAn1f8mkaA8riYaoCuWLiroypdXBG1/WdntbkFPZrKUhwwChqAnkeFgT8KZBF00tYKD3QmEEFRHg8442vuL931xU0zQlQCVh/3QnRMhd+vXspCf3mTmkC2Xj75tjid1rxgFk6LjwHJR742zKIffADIEDF9B5atienWE31x/FmfOq8k/3ZhCC7pmzLChqZvjJpeXzKwYzGpxjsu4IvRMFjNrOdvyC4tiqQzxlEEyY1IdLYP2WNG0SHd0DoVdDfM9dJnW6KfFToNUJfX/fMkiPuNauae+PEQkYBBPm0ytDDtpk/k569GgUXRR6lIRuuETTs8XtSrQoU6KXn7a4CtJetMW3ZbLGJ4Udb2Phi/PcnFdwPLnD96+sP7oDPAIGLuXUc0xxSPr9/PeH7zEfasbSh7jtlxKZaLI4+xJUVvQoXBZt/6UScbMEkvJVYBUhA6Fiy7f+vu3+OLDGzzbgnkr8PQlM04Xx2TGJBQwKAsajkCrW/Wg3+ep1gwHDM61hUJF8bIK1fvVLHWrrwS1LE/QhRAFYn+oEfpQ8HjOh7HAxWhgCG8xlLv/jb/EpOh4Yey+65pjhr5khs/fvx7ICW8x3ILeXWJ5N/dxKVf+uTqv205R7W37khmyFp6JTDcv72hjVV6qpIzQc4KeteTvWZblROiRoOHkjVcUWdpNcfHSKYBc4cgnIOz3FZTOlxJjJajOBcP1e/liPxL+r1v0jHFiuRi+4mP2G8Ij6MXuiMY6Y/dd1xwzbD7Q43S+K9Kq3MEt6MX6kCtylovp2DTxIoKuyvzVtqoik6LZrMWB7nhBD/Og31ew8EJvIkMma5G1ZIRaFvI7zb4qBxDTCxfL/PRI0KAyEiAUMJwsFSdDpcQFQR2Xb7lATmzLRjJC93stF/ci0WMVt+UScHWIDBq5NM2AIRBCC7pGc8i4PeuBMllSrknRjgEmEd1NrfKfw30HkN+3pTqSy0xRtPYlSZtWQctbWSnqFa3eRNq5mEjBN5wLyUARen15iLuuW8YPrzmdqkiAcMDnTNw5GSolI3R5XFVErr/pjsL9R8Ny8XkvIOMiQhfeKFxdfPyGwOcTGD5RtDBrPKAnRTWjTtwlsilzIG/cFaEPIOgqX9zdgEut0+kW6+Zeb5l/TZkUXfcFRuWH5y9dl792p3o+dacRyps0HczuuHCxtF0qwwESaRM7y3LQDBUlqNXRIHd/fIUnTz1nuRzepOhQyLdcxkMTK/dcp9/nc8Jad4aQfxzaLaAFXTMGcEfNA0XoQxV0d9qiQl003LrcnNcXpSzoly1xXeMplR8eKmG55CJ0g2ho6IKuUNkwqtp1MDFWizQHDMGK+XXefbbYLphUTtDweXqcDxfubJqA4eO02dV88vwFnDm3dtifa7jwTIoawik0ytktQ+tKORbRgq4ZddyC7hbhfIYcoRe5KBRLRcy3XFRLXPexpQRdWSogxT2ZydLnEvSQ3etFMZDl4ubmSxaSSGf50iMbgZx3PtikaDGLQG07eWY1m7+6ckQm+VQ2TSqTxfAJwgGDf1m5eNifZzhx59b7XfaK++I4WP79WGV8jlozrkibWT53/3p2t/UX3Z9w9aUe0EN32TGdA0XoRQqQ4kWyZ1ryLBc10elurVuqaZbKMweYWSP7h/cm0s7qQm6PPRwYesR3xpxazjmuPpe9MkiGSnAAQVceetDwjWjGhjOGcSKC7mF6slx8xStGxxPj4y+gGdfs74rzwJpGXtnZVnS/M3EY8g9subii31L9VmDwAiRlReRH6CE76nZbLvltbdXvuiP0mTXSyuhNFPfQhxqde57HP7QJzVyEXihAqs/LSIuT/yg9z3Dhvrj5PYVF6sIkDmnFpbHE+By1Zlyh0gBL2SkJJxPEXzS6Viixr3GVzBc9bgDbBqDW7rOS3xBLlfMPZLm4C3iUYE+vDiOEd1LUHaEfTv63WrhCeeelLgpKRAeyXEY6Y6NYu4GxjC/PQ8/vUKkjdM0xjWVZfPuJrWzc3110vxL0ZAmhjadNpxFUfoTe0pPgoTcaAZeglxUX9KauOB/52Wtsb+kbcLxlQT/15cGCNrwhv+FZhSiVybK7rd+xVNTvQk78QWaYlAf9/OnN/fzk+Z3O/iOK0PNTDktNig7BQx/pikd3c6vxQH5hkcrMcb9f2kPXHLNs3N/DD57ZwW1/2FB0v4rAS0XoyXSWcEA2s8o/5v41jdz8u/X0uHK8qyOBgiXkAD7x6zW8vKOdF7a1DjhevyGYXh0p2K4yV1SE/sa+TuJpk4uXTHGOcVdkBv0+bnvPEj5w+gzKQn52tfbzxKZm+1yGY8kMVFRUCmW5zKmLEjR8zLfb7OYzkIce8B+tCF0K5HiprMwv/XdXioJ8v/JXghovjM9Ra8YUj751AIC5dVJ0vvHYZs755jPO/lyEXrysP56SCw0EDF9BhK6WX+uJp0mZFkHDR3nYXxBdbznYw1tN8g4hP2c8n4DhY3pVoaArX1yN95Wd7fhErpITCnumfOzc+Rw3uaJgLVJ3WuORWC5z68rY/NWVBX3Qc69FeP4vdo6Rtg+O1p3AcOGuFHVXt+buNETBWq3jhSEJuhBipRBiqxBihxDiliL7ZwshnhVCvCGEeFMI8e7hH6pmLGJZFo/Zgq4ml+58YRdNXXFHwN0WRjESGdPJBMmf0OyK5fqtpM0sAUNQHvI7C1QoXt7R7vw8UJ8XkF9iFaGrvG81/ohH0Ns4cUaVsygF5AR9sMKZkNtDP8SFJcBrpQwU+Q4UhQfyhGqkyHno4yc+VH6/3yec5eXcEfqE9dCFEAbwI+BdwFLgaiHE0rzDbgPusyzrNOAq4I7hHqhmbNLUFWdvewwobGqltscHmRSNp0zbcpER+k33vsH3n94O5Fa8703Ygu73UR7yOwtUKDbt73GsjcEEPWgIZti+eH15MLfdidBlPvkb+7o4e36dt0CoSM8UgLs/dhZfff+JnnOVHUmEPsBScp7jhmC5jLS37UwmjqOoVkXphiujRb2H1541h2vOmjNqYzsShvKXXg7ssCxrl2VZKeBe4H15x1hApf1zFbB/+IaoGct0uZpWKUFXwrrDnpwcbFI0kZEeetAuUFmzt9NZCFr1UOlNpO0IXTa96k96Lx6bDvRw2uwahCiec+7G7/Mxo1pG3bVlQacBlvS9/cRSJh39KTJZi1m1UU9XRVW5qVa3V7ztuHquPnOW89jtoR/WpKgtOIN5ubk0ymK90kvbMcNJwPDhE14rY6yjfHQZoXsnRT9wxkwuO2X6qI3tSBiKoM8A3E2qG+1tbr4EXCuEaAQeA24sdiIhxA1CiNVCiNWtrQNPXGnGB+7JSWWtKDtjpy3o8UEmRRMpabkEDEHatEiks453ri4YsqxeeugVYT8pM+tYOsmMyfbmXk6YXklkkCXTQIqkGmN5yO/8jorQk5ksbX2y6Ki+POgp8c/30N24I2F33/PKyGFE6EO0SwayO5xS9hG2QgKGb1zZLZCbwPUbvtw6rOPUZnEzXH+Fq4FfWJY1E3g38GshRMG5Lcu607KsZZZlLZs0adIwPbVmpGnsjPGDp7ezvqGrYJ9KH6yOBpxIXLXA3dGqInR5zEBpi+5J0WTadEr78y0Xdx9yFaVvb+4jk7VYOr2yoGFWMQK+XJZLmVvQXQ23GjulXVRbFrIXnPAuxjxY5WfI72NqZZil0yo5ZVb1oGMqGKM/5+cOxKwamQUzpTJcsC/gl50DRzpyDvjHn+es3hJ/kUrR8cxQQocmYJbr8Ux7m5vrgZUAlmW9KoQIA/VAy3AMUjN6JNIml/3wZTr6U+xo7eN7V53m2a8EfXJFyPHKVUSuLJd4Krck3PbmXiojAY8AJdLSQzd8wo68syQzKcys5fjhjoduCCfy7UtkqC0LsulADwBLp1V6VhEqRcDwUVcWJGT78ZGggYjJCC1iR+ONdsl/ne2xR4N+uuPpXIReQmj9PkEmazmrDj1207mDjqf4ebwTdaU4ZVY1m75yaVGfXEbOIy+0QWP8dSc0XB56aIjzFeOBoQj668BCIcQ8pJBfBVyTd8w+4CLgF0KIJUAY0J7KBGBHS58TLfcUmWxU6YOTKkLs6/BOju5q7ceyLGdh5mTG5JO/Xcups6r57w+d4pxDReggc9JVpsv+rrgT7bs9dDXJqC4malWgmTVRooHBP9J+e/GCL/7NUpZOq2TN3k6Chg8hhNNQq8F+LXV2VWlZ0KAvmSE8SL/vP990Ls9ubTni3G+nr/gQzlNq0vPCxZMRjLxIyayQ8RXdKkF3d1Ycrz3Q3Qz66bcsKyOE+DTwOGAAd1mWtVEI8RVgtWVZjwD/DPxUCHEzcoL0OssaaO0ZzXhh68FeAKZXhemMFQq6EtUpFWG2HJDHxm2LJZ42iadNT+l/Zyzt2CiKRDpLKGCQzVqexZZ3tuYqPnsTGVKm5UyKup+7K5amPOT3VG8OhBLJj9iZDNGg4URpynJp6Izj9wkq7QnNaMhPIJZyJtBKpS0unFLBwikVg45hMALG0CyXgTh34STOXTjy1qb/KN0JDCfeCF0tcHEMCDqAZVmPISc73du+6Pp5E3DO8A5NMxbY1txL0O/j1NnVbLYF201/MoNPyGwRtaZmPG1SVxakvT9FXyLjyXJJpM2CDJWEHaGnTNOzKMXO1lx3xr5khrS9VqfysfsdQU85+eRKkMuCBv1FWuZCoY0RDhgE/d6+47vb+qkpCzr+c1lQZuFctHgyN120kBlFKk2Hk/FUrFMeMoY0dzGWcGe55O6Gxv57PRjj/5KkGVG2HOzluEnl1JWFCiJrkJFzWchPNOQnnjZJZrJkLWnBAPQkMp489Fgq41k1CJSHLj1n932ditCFcFkufuEIuhL/rnjaWW3IEXT7mGLLQuZHYu4IfZ5dYr+7rd+xW9T5gn4fkyvD3HzJ8SM/0ej06B77X9EbL1zID64+fbSHcUj4injoEyFCH/+vQDOibD3Yy6KpFdREA3TH02Tzyur7khkqQn5HSJXfPtme9OxL5gS8L5kha+GJnNNmlkzWIhIwCnxplfY4tTJMjzMp6nOiaBWhd8ZSznqg4YA30o4WSWPM96UrIwHnIjGjOuL8XOcqOooG/UfVYz1heiWnz64uOv6xxvTqCCfNLN6aYKziTls8pjx0zbHDhqZuWnoTzvqW3bE0B3sSLJpa4UTPPYk01dGc0PUlMpSHc4Ku8scn2xF6byJN3F7AQpXxx2whzphZZ9GLcMAoKPtXlsus2ih9iQwWeDz03kQay7LojqUdC8Qp5rGPCQcMEpksZtZyMlDy/d7PvXMRPbZ3L4Tg+CnlrN3XRV1ZyDmmrizoaRMw0py/aDLnL5o8+IGaw8JtudSVBwkYgimVoUF+a+yjBV3jcO3//ZWuWJqXb7mQGdUR9nfL1L3ZtVGnY2JXzCvo/Sm5gLEqvmnrlwU5StD7EhlnkrTHzojpt732D9/5Gmv2dgIQDhoFFZ5tfUkMn2B6VZg13Z2E/NLHViX133hsC6t2d8gIPaosF7/n/6BfTtiZ9l1AbzJTYGPMyltrc9HUStbu63L6pgN8fuWignYDmvGL23KZXBHmtVsv8vy9xyvj/x5Dc1i09yVZ9rWnePt/PcOrO2VjK9Ne4u2me97gq3/a5FRpVkUCjmDm++iOh14yQs8U9HiJpTL88c0DjpgDhO287XymVISoCAc8eejuZlWr93bSHU9TY19kVJZL0O9zWtyq84btfYN10ls8VWapuPu81JeHmFuiha1m/KEidDXpXFceQhSbcBln6Aj9GGVXW79T3v7qrnZWzK8la89Irt7byeq9nU6XwapIgJQpxbArLxe9L5lhenXYEdJ2+5zKQ+9NZgoWaE6bFo9vOOjZFgkWeugg29O29SXpTWScalI36qKj7hqirjJ+JeoqqyWS17OjFItsQa8tG/+34Jri5CL0iRXTTqxXoxkySngBumMpumJp+lMmt71nCd/98KlArrimMhyg2vaPu/Ii9L6EbbnYYtluT4pOcnnosSLNspq64k4kDBD2G560MXVHcO2KOVSEA5hZi95EoV3iHG+Pz4nQDblWpDtCV3cRg2UznDqrmqvOnMU7Fun2FBMVd/vciYSO0I9RlPAG/T6642mn1H12bdTxEvfagl4VCTjRe1decVF/MkN5KJDz0O0LRXnIT1nQoKNflvAH/T5Pc679XXEWTa1gi124FAl6o+/ffuwsZtdGCfq9laEqQ+X2K09hb3uM79ltdnNpi/LYgL1WZNDvcyyWXFe9gb/E4YDBNz9w8qDvoWb84vbQJxI6Qj9GUV73/PoyuuJppxnVzJqoY1/ss/uZl4f9VEYCCCHTGLcclL1TslmLvlRGFpaEvGmLkYBBedhPa68U+JqoN0OktS9JVSTg2CAqD11REw06bWfd/cSVGF9x+kz+9uxcz+qqiPLQc6X5OcvFZ49p4qSnaY4MdU0fD4Vbh4L+ZB9DPLCmkavvfI1YKkNHf4qKsJ9JFSG6YrkIfUZNxLE7GjpjVIT9GD45EVkR8nPv6w38/c9fByCWNrEsiqYtRoMG5SE/LbagqzxxhWVJW0UtwGz4vOs4hl3515PKc162W4xry4LO86oLRiTgd45Tk6LKQ1fR+3jrDKgZfgztoWvGO/e93sCru9r5j4c30taXpL48RFUkQI8doVeG/TKjxfaj06bl9DKBXNrh/u4EHXZZPyAtF1tIlTcfCRpUhAM022ttVkULc7irIgE+bC8KURMNeAp+woHcz1NdS8C5BV0IwawamXLoTIoGcxOfU6vCTKkMO/3Ahzopqpn4+MTE9ND1J3uCc//qBjbt76EvmWHtvk6qowHuX9PIpgM91NrFMl22hz7TFke/4XMKc0oV02w52ENfUvrp5WG/MxnZ5rJcKsLuCL3wPNWRINe/fR6vf+Fi5tSVeVbdCftzEbpb0PMzYVSEn9/LJeT38b/XnsFX33ei01s8PEQPXTPxMSbopKgW9AlMIm1yy+/f4pev7OG1ne1kshZ/e/ZcQLa2rSsLUm2X9Dd0xhxxBKi2Jxndq+38+aZz+c31ZwHSS++zm2xV2H1O/D5Bym6g5VcrC9kToTXRwqKNqmgAIYSTEeNepcfdKyUa9DtfvPyy/aXTK5lVG3G+oBFXhK56nave4spf1xG6Jlf6rwVdM07Y3tyHmbVo6U3w0o42wgEfVy/PrVVSVx6iOhLEzFrsau13InTIed5uy2XJtErOOa6O2rIgWw70Ov3Rnb4ptpgqUVU9UYCivT7yo3YltKFA4cdS9VXJj64/feFx/OnTuUUkclkuuXME8vLQJ8LKNJojQ3vomnHHpgPdADT3JNl6sJcTplcxrSridBF09yfJZC1vhG573vmWixCCxVMr2HKwx7XupoywVY8VJZzlIfm7M2siTq8Vd5pYdV7U7lR0FmlIpfqq5Oehh/yGx5+PuipFFf48D73YgsqaYwtDe+iasU4yYzoLUgBs2i/TC1t6EzR1xR1RXTilHJBRr1sM3YKuLJLKIt73oqkVbGvuc/xxZZmcMacGyN3GqnTDU2ZWOwLrTl/Mv1ioNqbhASP0gT+yFWE/Ib+P2rLcudV4VOm/jtA1Pm25aMY6dzy7k5Xfe4E1ezsA2GgLeltfigPdcWbYgr1wsiptD3psD4/lUiJCB1l8FE+bbDvYSziQW7T5itNnALn1OFWzrYVTyh1Bdy/KXB0tbrm4J0QV6i6AQdbBigb9PP7Z87j8tJnONuW7R3WWi8ZGR+iaMY2ZtbhvdQOWBbf9YSNpM8vmAz2OBZE2LSdCP96O0OvLQ54IfYbHclEeemEx8bQqedybTd1Mqsg1Ncpf7kx1aFwyrdKJviMBgzK7CCn/YqG87uKWixxPR5FFNvKZW1/mtVwMneWi8TJRPXRd+j/OuX91A3XlQXxCcKA7wcoTpvKXjQd56I0m+lMmlyydwpObmgEcQT/v+EmcOquaJdMqnSwUlYOuUJF7sfzx6dUyjXBnax+nzqp2tgcMH7/4uzOdL8vNFx/PzJoIFy+ZwvaWXIl/NOgnHMgUCHfOQy9mucgI3d2DZqioStEa+6JQES6eiqk5dvBN0LRFLejjnP9+fCvTqsIsmlpBZdjPbe9dwl82HuT+1Q0AXLBock7Q7Qh8Tl0Zf/iUXAJWRdFuuwVyvVEqi4ifitAty2WF2LgXZagpC3LDeQsAnIV4IwG5/mR+5ag8pvSk6FVnzuKVnW1c//b5pd+MEqheLuctnMR9/3C2001Rc+yibtImmoeuBX0c05tI09KbpDOWoiOWYvm8WmbWRKkvD/H6nk4Mn+DchfXO8dOLLGwcDsj1NN0TogBTKmQUriY83dSVBQkaPlJmtuj+YigLJBo0SGb8nrVDFU7aYhEPvaYsyK/tHPhDxb3g8vJ5tYd1Ds3EQlktujmXZsyglm9LmxYNHXFOt7NMTpheCcCCSWVMr47gE9KvdueFu3n7cfUe4Qc4e0Edv7thBSfNKMwf9/mEU72ZH6GXwh19nzm3lrMX1BUcExggy+VIUJbLRPvyag4fNS8+0TKedIQ+jtllr7mpWDZHRp8nzqjk+W2tLJ1WieGTlZh1AyzW8H/XnVmwTQjBWfMLRVcxtSrMvo7YYUXot7xrcdFjBpoUPRIChiBgiAmxIo1meJiolaJa0Mcxu1r78Ak5YdgVS3GyXY15wnT5/1I7Ul88tZKpleGS5zkcptsR+qTyoa3DGDRyWS6lUE20hjtCDxq+CReJaY6Midqca0iCLoRYCXwPMICfWZb1zbz93wEusB9GgcmWZVWjGVF2tvUzqzbKuQvrOdCVcCLbs+bVcsqsai5cLCcof/bRZQz3x3aa7ccPNUJ3LJdgaUH3+WQkXSwP/Ui4avks5+Km0YA7bfEYE3QhhAH8CLgEaAReF0I8YlnWJnWMZVk3u46/EThtBMaqyWNXaz/z68v42vtP8myvKw/xsJ3FAiNTSKPa1k6uGFrkL4Tg3969uCBXPZ9b3rWEs4Z54vK4yRUcN1lntmhyqAg9MMHu3IYSoS8HdliWtQtACHEv8D5gU4njrwb+Y3iGp3GzrqGLG+9Zy2+vX0FlxM/Olj7Oy5vMPFpcftoMJleEmFUbHfxgG5XCOBDXv33ekQxLoxkShk8gBJ6unhOBoQj6DKDB9bgRKJo/JoSYA8wDnjnyoWnyefTN/TR0xPnRszuYW19GysxyuV1uf7SJBA0uXjplVJ5bozlSDJ+YcP45DP+k6FXAA5ZlFS7zDgghbgBuAJg9e/YwP/XE5+Ud7QA8uLaRykiAs+fXsXiq9oY1mkPl0hOmFPTWnwgMRdCbgFmuxzPtbcW4CvhUqRNZlnUncCfAsmXLBmmzpHHT0Z9i04Ee/t+KObzV1E0ibXLTxQtHe1gazbjkjDm1nDFn4hWZDUXQXwcWCiHmIYX8KuCa/IOEEIuBGuDVYR2hBoAXt7cCcPnpM/jq+08c5dFoNJqxyKD3HJZlZYBPA48Dm4H7LMvaKIT4ihDiMtehVwH3Wlaxom7NkdDYGeOrf9rEvPoyTi5SuanRaDQwRA/dsqzHgMfytn0x7/GXhm9Yxw47W/to6Ih5mlol0iYf+vGrfPjMWVSE/XzhoQ0A3PPxM5wydo1Go8lHV4qOMt95chvPbW3lrS+90ylNf2BNI281ddMZS9GXzLBgUhm3f/hUFkwqH+XRajSasYwO90aZbc299CUzNPfIPt9pM8v/PreTsqBBY2ecrliaf3v3Ei3mGo1mULSgjyJpM+t0TNzV2gfAXzYcpKkrzn998GQqwn6WTKvULV81Gs2Q0JbLKLK3vZ+0KeeQtxzsJZnJ8rOXdjOnLsq7T5zGtKowVZGg7hKo0WiGhBb0UWR7c5/z83ef2kZPIgPAF9+7FJ9PTMg8WY1GM3Joy+Uos3ZfJ8mMLKTd1tyHELBwcjk9iQyLplTwrQ+czEdW6CpajUZz6GhBP4psPdjLFXe8wsNv7Adge0svM2sinGjnll+1fBZXnjmr6BJsGo1GMxha0I8iT2+RizXvsidCd7b2c9ykck6cUUU44ONvTpk+msPTaDTjHO2hH0We3dICQFNXnGzWYk9bP29bUMffnj2Hvzl52pDX59RoNJpi6Aj9KNEVS7FmbycgS/mbexPE0ybz6ssIGD4mD/MScRqN5thDC/pRYu2+TrIWzK8vo7Ez7izwPL++bJRHptFoJgpa0I8SWw/KFMWLlkymtTfJ5gM9AMybpAVdo9EMD1rQjxLbmnuZVhV2Fit+aUcbkYDBVG21aDSaYUIL+lFiW3MvC6dUMKNarsH54vY25tWX6SpQjUYzbGhBPwqYWYvtLX0smlLOzJqIs+34KbrhlkajGT502uJRYG97P6lMloVTKphSGSYc8BHw+fiXlYtHe2gajWYCoQX9KLD1YC8Ax0+pwPAJXv7XC6kIBwj69Q2SRqMZPrSgHwWe29pKecjPkmkVANTpAiKNRjMC6BBxhDGzFk9ububCxZN1jxaNRjOi6Ah9BHhqUzNNXXHOOa6Otr4U61vdvgAAIABJREFUHf0pLj1h6mgPS6PRTHC0oB8hT29u5l8ffJPHP3sedeUhGjpifOI3a8hkLfw+QUXYT0XIz/mLJo32UGHTI7D5EfjAz0Z7JBqNZgTQlssR8pvX9tLWl+L5ba1YlsXtT27D8Ake/tQ5/M0p05k/qZz7//FsykJj4Nq5/Ql4637IJIfnfGYa1v5K/q/RTCTatsOel0d7FIfMGFCZ8UdnfwqfT5DNWry4vQ2AR988wL2vN7BqdwefeMcCTplVzXc+fOoojzSP/lb5f18zBKJgWVB+BHcOO5+BR26EYBmc+IHc9mxW/u/T8cIhod+3scPz/wVNa+Eza0sfk80CFvhKzI2Nwt9TC/oQSaRNrvnpa1y8dAq/fW0fPh+89+TpZLIWJ0yv5OktLfgEfO39J3LVmbNGe7jF6ZPte9n0CDzxBfnz3z4C899xeOdr3yn/3/msV9B/cwXseha+1H34Yz0W+c3lMPkEWPmN0R6JJt4JmcTAxzz7ddj9AnzsyeL7H/k0pGPwoV8M+/BKoUOBIfKnNw+wdl8X3/rLVg50x9nfleB/n9vJxUumcMN58wH4+HnzuXbFHPzGYb6tiR54+FPQ3zaMI3ehBH3bX3LbmjcWPzbWIccS6yh9vs7d8v9dz8loX7Hr2SMa5mHR3wZ//CykB/kSjmXad0H79uE/b6wD/ngTpOO5bW/8Fjb+ARpXw3P/BfEueOQzkOof/ucfT7zyAxmgJHoGtxLbtg789+rYDW07hnd8gzAk5RFCrBRCbBVC7BBC3FLimCuFEJuEEBuFEHcP7zBHn9/+dS/z68s47/hJfO7SRfzn5Sdx3dvmcsdHTufdJ03j2x86hX+65Pgje5LdL8Abv4Etjx7678Y6oGVL6f2WBf22oDeuBn9E/utulNs690LP/tzxO56WY3nkRmhaA8newnN27pH/dzfkovVDJZOCfa95LwiKWIf0MofCnpdgzc+h1X4PWrYUXowsS772scb+dXJeIxP3vs/ZLDSu8R7buRd6m/OOWV3kmIO5x/tegzW/gIMb5L9UDP76v3Lbpj9Ie6Hhr7D2l3BgfeH4kr3Qsnnw12Fm5GdlPPPit2Hd3ZDsATPl3de0Vr5GRaIHkn2UxExCaoD9I8Cggi6EMIAfAe8ClgJXCyGW5h2zELgVOMeyrBOAz47AWEec3W39dMcLr8rPbGnmjX1dfGTFHH7198v55PnHceWZs/jSZScQ9PsIGD4+cMbMI88zb9sq/9//xqH/7rNfh59dXHrCM9Gd+4Bm4lAzF6pnQY8t6PdfB3/4x9zxQdlEjC1/gp9eCC/8T+E5O3bDtFPkz5sfkf8XE+aB2PJHuOtSePorhfteuh1+fcXQzqNeWzYjb5fvOAt+f4P3mD0vws8uGpo4HS1iHfDTC2DDg/Luwi3o2x+Hn10IHbty2x74e3jy33OPdz0jX5M7EvzeyfDtRbnHWVuEkt3yud74tXyubEZGoZaZE55iUemvr4A7Vgz+WjY/LD8r3U2DHzsWsSz5PUl0SbHOusS796B8bZv+kNuW7IFsuvR3LpM66nc8Q4nQlwM7LMvaZVlWCrgXeF/eMR8HfmRZVieAZVktwzvM4WXN3k6+9ZcttPTK2/Ns1uKWB9/kgv95jotvf56fvbiLzQd62Nbcyz/ft55/um89i6dW8JGzZo/swFqPQNAbVkGqV0ZaxVATooraeVA1U0boWRNaNslo8OXvw50XFEYnamyKrAlde2HeO2DuuTLay2YLo+KO3fCDZV5RcpOKyf9fuh0OvuXdl+iW4jwU1JfKTMP6e+XP6g5Coc6VKOLtv/IDuPP83ONsFn7+btj2+NCe/6kvw1NfGtqxbpI9YGUh1m5H6D25fSrKTri2JbqkPaJQPxd7TYpsOneMmbKfKyHfKyXg6jxuQX/iNnj6q9C4yj5PduDXooQ8PoBNN5ZJ9cm/RbwzF6E3roGfnAe9BwDLexer/i6lonQzedQFfSiTojOABtfjRuCsvGOOBxBCvAwYwJcsy/pL3jEIIW4AbgCYPXtkxHHzgR62Huzl/afNAGBDUzc/f3kP171tLifNrCKZMbn5d+vY1xHj16/u5WcfXUZnLMW9rzdw9fJZrGvo5muPbnZyyNOmxYLJ5Xz7Q6cQDoxwpacSzeaNUqD8Idj9ovyALb3Me6xlyVvkGWdA3UIpyCAzT+adV3hu5Z8jAAtq5skPbfNGKcyZBJDIRX/qi33OTbD3lUJx7NkvP/C182D6qTJy3PkMlNV7j9v8iPQZ97wMtfMLx+WOgrY8ClNPyj02M/JLMRTUcdk0rL5L/lxhF3MdeFNeLIygfFwsonriNu/jTBz2vizfy+MvzW3vb5Opn8tvkNkN6++F2WfL98gaRPCKocaS7JXvhTtCV+LuFlkzlRNo9++rbcVeW9a0z2cLTzom/5mp3IU7oQTddSHf8xIIV8yXzYAvWPq1KCFXF+nhYPXPYfF7oHzy8J2zFOqiGGu371gEHFgnbaguWwLdFyv190n1Qlld4fkySUj3ywvhUcp0Ga4sFz+wEDgfmAm8IIQ4ybKsLvdBlmXdCdwJsGzZskO8N7fpa4W+g2SzFuubulk4uZxyO8e7M5biq3e/QWd/ilD7Qh56o4mGDvnh+nHDOj578fE8vK6Jss4WfnjhcTy8rolbf/4oscgM5tWX8dV3zsTohY5YlB88vZ2tzb3814dPZva0qVBT7jw3AHXHQSAihbV9hxREf1huT/ZA1z6oXZCzLhQ9+6VA1y8Cw2//0eMQqoS2bVA5U9ogzRthxulSYA+sh8t/IsWlYqoU1w0PSpsiWg/v+bb9ZQvAtifgjL+DmjnQc0B+OOuPz/nntfOhY6cU4liHTGE88Gbh+6wm0M78GCDg1R9JgTXsj4wS+Jp5MOcc+cXf9ypMP839B5ciDzlvOx8lVtVz5LHnu6ZosmlbwLJScKK1xX8/k5C3tyBfb9u23M9dDfCTc+Xj993hfU6QAuqeSLUsEMIV8adyAhWMyvfhpdshWA6nXA0P/QOcf6s8zsrK9yjVB5HqwrGmYoAl0zwVKpNC3T0k++RrySRyEaCZlBF0qNK+yLkF3nVnAvLvWew9Uq8V5OtNJ+T7qy6oSszcgq4+m4psGhhA0NXd2aH4xoluCJTlPldu+lrhT5+V78WKfyzcf6Rks9KGitTIsau7FMcysnJ/H/Wa3Hegg0bo9nuZ7odQxbAOvRRDEfQmwJ2HN9Pe5qYR+KtlWWlgtxBiG1LgXx+WUbpZfzc8+UV8wGl5u2qAuwFCwEvS9Ef1weoFHoLPAZ8LAa/AewEM+Oe+T7DyXZfh/+FpkOiiDviSOul99v8rPikjP/UHrl8k05Ve+G95u644/W9hy2MQa5Pi/rGnc1/u7U/C3R+WnuX5t0rxeuI2GZn+7SMyajrrE1IwGlfDlBPkJJaVhd9/XOaOf+wp+N+3yfPNO09OqCmv+LRr5cTg906GT62St4qZBCy7HibZnuq0U6Sg18yV54NcVooRygmEulU0gjBpsfwyd+6G+oVye7cdsdTMAX9QClyqD3pcH41kD+x9Vf6sRDYf9aE/fiW8/jP5BQ9X2ftsIdrwIPz+Y3D9kzBruff3X/ouvHmvfO3gjQ5j7V5v3nSJtOI3H/DaVGZavh51jJmC2xfL7V84kPtbrv0VnGj7+5mkLY4m/OUWeP2n8O9tYAS8Y334k1JUr30wt825ENlCkU3DM1+RVs/8C+S2eBd85wS47Pt2VJ0u/H0VobsnQxVKtFP5EXo69zqVmLnvmDIJb0Tq3lcMJ0Ifos1gWfCjFVKsz/lM4f50v3fcw82WP8kL8t/9Ge58B5xg/z3dd4Xq86REW73GTNL1XSkxvozruzSGBP11YKEQYh5SyK8Crsk75g/A1cDPhRD1SAumhGl6hCx6D+v7a7njuZ1csHgyAuiKp1jf0E044OO6t82lM5bmD+uauPni45ldG8WyLO5Z1UB5yOCiJVMpC7msk7/+hP9puAvx6gMyyvzgXblbc+cd+Bm8docUwUu+IoXisc/DHWdLATv1I7DoXbD5T/KLHqqClf8lxfrHb4eofTvWth2m2PPJ2/4C7/hXKf49TXJSE2DhJbDhAZkKOHOZ/KJe9gM5w77m5zlhvPhL8iKz63m4+0oZqV/6DRnBP/efMoLOJORrWW8LnjDk82/8vYysleDseAYqpskoueE1uc0j6PbFoHVLTtBV5Bawo81gufzQd7vcud0vyA992eTCCH3LY7B/be6icvw7YdVP5G3+4vfIbUpAlJ20/p5CQe9plBGVEra0/QWM1nmjdfeYMwn4w6fgjOsK5xx6D8i/2zv+RT42015/Wglg46pcZo8SWTMNGx/Kjfn5b8F7vyv/HidcDq3b5Hj3vQZrfw3v+6ErQncJZ9Naee7pp8vHsTb7YnnAvnAMEKH3HqAAZ1LUjtAT3YDl9dCLWS7pRN5rH0TQY2qOogseuF5+vicNkPmV7IXe/bn013zUnZP7LmE46W6Qnxc1Sb7x94XHKLFW9op6jR5rrFSEns7tPzp6PrigW5aVEUJ8Gngc6Y/fZVnWRiHEV4DVlmU9Yu97pxBiE2ACn7csq31ERlx/HOvK/TyereXrV1xMvd2K9mB34v+39+VRdhXnnb96e3er1VpauwQSkgwIIwQIzBgsYjAYCJYwMRwwYzYPMMkQ2yFDjMMcvGU59pxxZhxjTwgm4CXBHBJnsBEQG5vj5MQswoOxMBYIm2EzaINGrVbvNX9897v3u3Xr1rtv6e73nup3Tp/X775761bd5Vdf/b6vvkKlmMOcbiLjU8+fDOPBFYAPr0sp7/BToR66mW7YqZ8AVpyU3GflacCP/ox0Uya3rnkU3rTuAuCszxI5rn0/achHngesPBXoW0ahfxz5MX8NEfFTf08v+UuPRlEmTLJLjyfL7JnvRhN+Vp1Onc2TfxfFqK98D2ns7zgb2HIrab6lbmBF4N7gB27DZXTc//0W1e2YC+klnXcESQsAMPASneOUPwB+cAtF2/CDnC+SZANQx7D/dWDJhuhh5U6hPIu0xDFhnbEsc+Q51NGNHiCi6ppL1tFzD9E1BYDF6+nz7ddIbir3RudgiYKtfYnxkcDBFxAbv/y9S4jQpTOXX7yhvcBT36I/vi/jo3QdXnmCdP81Z9JvkuCGB+IjAC6bLd2JMdJ6h/bQtfrV90l/3/Z10tsP7KIy/u2vqEN/36cjK04O5ff9hkZxPNrhesuOQ7af6wAQ6QNARUg+poU+tFfUWzhMzfaaE2uyWui7fkVGyYqT3YTOjnrToTsxRoYKS2xjB+mZKPWQUdUo3nqJ5BJ+VnIOGhw1RgncRlnnUUtIL1Ddgp8CZNLQtdZbAWw1tt0i/tcAbgj+phx7B0egFDC3O7KkF/fFF1vOPLmnex7wwa+59+maSzq1xDEX0F/spCXg/X8efT/6A/RnYvUZwCN/Afzw0/T91I/TRI9L7yZdfvUZ5PB84nbqOOYcFlmyTOhFoc0ff5moQ6AxMaEfdgpZIC8/Sp3V/NXA2X9Gv/Utp7YdfJP2O/Ic8hF87+OC0EtU5qJjiaB+9X1ywp5wefQ7EFnoY4Lw+GVgst7zHEWS5EvAUedHRKjyUb0nRklCWnRMZImydbT7WbLaiuJejw8D0MLhx4S+GHhjO3V0bK2HTixDEjjtj4ioH7gx+o2vnyTP3c/FrcURQYLcFnYKv/iv9PlSMAIYeCW6d88HMwsHXrZb6PuDSAruEEcloZuSi+EUZQtdDvFDC93QgaW1b4tyMR2scmRgA/sBuP4H30rfF4gc9Sah79gK3HM5jUwBuiffvY5GkJd8211mFvzg0/RO8EgwjZD53EDy2sloJJuFrnXUOU5jpEtbTv3fc2AU87pLyOfadIHlpceTTvzyY2SVn/U54IxbIsfQqk1kke95Dlh9JlnSbKWyVWM6Wxl5g9DzReDK79PLZTrqCmXghmfpgWNZiI/nhzAXWODX/IhevAdvIucnv9xM6OVAQ5dDUSYiJnR2vk6MRhrkxCiVweVMjNHLMnog6bADSO//zn8keeld14kokeAFGxeEzpi3WkQuIGkx9S2PwiqZsIctESa7fxUfgUjS5zb1BNEYr2+nT5aw3tgOIBip6SDqZOCVqI1DlvBMttC5vuMjtL9VcgnKYQ19chx48i7gqW/TiFGWE1roonMYNghd6+haMlwWutYR2XEdqoWcsgPXJHSeOMWjjbGD9PyqJkWK7H+dzsmdqW3SHMPU8Q/uC+LVBaHbLHA50plGQm/Lqf97B0cwf5bD297qyBeAi+4CzryFold4G6N7XvQ7W9OhhR4QelFESsTKDgg4tLDLtG3WgqSTDqARQU9/JL/I43OFKNyqUKIyKrPjRMD7l3rJUhkeiF48fpAXHEllPf8v0XnHDwakHhA6dxwsA0yMRiTFL0+5j6b3T46TTwOIXkreR0ouDA6X5BfXtKj6VkTn5xc4DBkcBWYtov/37Ihb6MNiH/4Lh+8Bee8NJvy89RISGHhFOM4spMJhkDLckM/HYN8Bb2MLfXyEOpHXnkpq6Ey0MmLGlFwmx5NhmC4NfWwo6ly4DsNvAU/fIxzjO+MBBPwsm5Y8j1a44xkboufR5vCtB0N7Iscw4J7tmRixjdJx1Sx0ObpxjQCajLa00PcOjmJ+T5sv47b6vfSXBjPuvGRILmkWeii5DMa/ZwVbyiODSecw/y7JizsC1tCHB4CeBWR98ctQ6iGd+gURTTM+AkATQeaLQcY6FdeJpeSi8sCJl0eEsCrwL5gWOr+kTMIAyUzympgv2OylUcfEhC1fYI7j3r2DiL9kjEa485kYrZ7QKVekdqkcOXOLXe79geTIQhLrhCm5BKTH4ZwyNJHryyMEGdMeEvpY/DpIuCx06QNgy/rgWxSdBVCitl/cQ2kGjvswxW2bksvoAaojlzUUPOtjQ3TvJsfjobP14gATuhGSCNB9kR0Z+0zkPkP7DAvdQtjeQs+OvQdG29tCrwdFQ3IppBBBSMhvx79nRUFILjaLPl+MLGhZdmkWcHCAHvzu/qgMKCL+/ndE1m/vYhHfG5xHKdFZjCHusHubYvxPvCqygPmlC4fNDgt97qpgH4uF3t1PpMrl8gssCY5fzr07iQi65sbPyftMjlcn9FWbgL7DqIMbeDlbbvqwIwrqFptYxBb6GEkBPJNxYpR+05NiFGBYkpOibXw9w/QQtglKDg1d+gC4kxky4iJ4ZMCBAAcMQn/kL4Gvn5W00IcHgnOLfET1YnKC6jExGhGxtLZnL4vvH0a5CNI+uC/6rvIZLHRP6E7s2T8SRrccMigJyaXYnT7zLCT0/fHvWRFKLgfSLfTxESIQSfjlWZGTkB2Do4GVrxTFsodtmRW3jricfCmwwkaTk14KZbK0r99GEgmTPb84puRS7qVOsGch1Q2IXmBJbH3L4+22SS4yVnvsYOSLCC30kWgf+XLbIjK23Apc8zC1QUouLoQW+oGoTgzuQCbHyXIc3U/y18RIpIGHnZ5J6MYkJVm2rWPKaqEzzHBEc+LOYGCcTIzQ87BnJ0lTPMo4sDe+H2APy6wFB99EKIcNiclcjNnLiKQZNllmaF/0fPQuTtHQJaFPX5RL2xH68NgE9o+Mo/9QtdCH34pHuJgILWyWXGol9GD/0f0phF6mITvHuDNKIqqiR1joXB8O9wSCmZ2CZLicfCGuR08IyaUQRLbMW0VlmsRjWuj5Ig3r+5bHZST5CQhC53bzC2w4PHnb2IHIQg87kSDSRtYDoHBXICL2QoUIYNZCCmmtm9DHifh+/UhUt4mxKCqGQwV5f74mNmIxrcfJcWDHg/H5BAxTQ//NvxIJ736OtHIg/hyY+YPYQh8wLHSAOm0ma56zwBa63K9RHV2mpubypfXdNScePGCGLQJkoQ8P0HtYmZNioYtO16XRNxltR+j7DtCFmn+oWujm/yZM8srXqqFnkFwAIr6cYaEzehYky0gQupABQkIvEfnoCSKPSSm5iHbkS8ncJaaFnisCh70bWPWeZCcnHcZMuqFTdChe3vhBkiOK3XTOoTejGO8wDFK8sCP7SQ7rW0HzEgBgeTC3oWdh5HPoW0FEJTsAILIOZacdOkUDcpkcAx79GnDPFfEUBWwR9x8Z1UVeExlSyjC3jewH7r4UeOxvkvvGomvGgH+4lGLq77+BZnADlMEzDRxJw53F4K7oug8PCMs86AhYQ5ejhUYt9CELocv7V+mLx/CbYYsAdaYjb1MqBvYdmZjwkksm7B0MCL3nELPQCxXQFCmkR7gAyTj0qZBcwt8FoZcEoXdLySWoz/y1IjnWcCQHmJKLjOTgIf7YgchCB0jvDiUXlhzYmRccny8AF/4NhYSaoZz8efWDUY4QdrSNGRY6v8jcpsE36EVW+aTlzMcdcTrwR9spaRkQTfaSy/2xVmsmPePRDTtyZV2k5DI6GGQEZKfoOE1IAoD+NfG6m+GHEmYiLc7+OGjRqqXk8soTRGTjQR4is10SnMohlFxeIb1/cFfUzqG9SX3czPgJNNdCZ61+xCD0WYuikUZ4j/cj9Af98DM0Ua7UE82/MDHunaKZsOcAPcCHnIUuY9FdFjpbPGw11Cu5mJKK+bu0rAHDQheSC5N1sQJceT/NXB0TFnpMcilGD//kWHyIb1roac47Jng5eiiU4r+FcpQoMydGHkCSRDmb3uQYXf9CRYwKBCmOCKlqzmGUo+f4j1CcP8eoA9GwPpZMS1HHUeqNO3XDYb8ghrFg5CCloTd/A8xanBxBuKbO2yx0ILJky33Rb1Jv56RrE2MUodSzELji+5EcJcH1CZ2ir9J5xg+Scxggh3OWbJXNtNDN6B+ACH3zXwMf+J/0neukJ+meX3Yv8B+up237XojmX5iY8GGLmcAW+iGnoQM0DB8ddGvouRyRU92Si7iutinRaRZ8mobOE5YAmgr+3ENErCw9mJJLaIUaOUukhZ4vJS10RmihC0I3r0E4epGdhCG5jBu6M1voAEXFFEqC9MULPTES7yg4fcMpvx+RFxCNaKTFWKiQM7dnPs0QZlhHAjzRJSDJyUBDn7cqGenkzIViJD3lNrHsccJHaNLVjq1EgK88SVlAOQR1YozuZe8ikre235s8BS9IPiwsdJ6TsPI9NPs4LRunRK5IFvrYME26W7K++jESe3bSak4mmHBPuIKSxPWvsctT+VIUblzpo7/fPp200F//RTzCx88UTcfewUPUQgfIMjyAePpVGwrl6MWsNQ5dWvTVJBdZtk1Dt1n5xQpp5Mwj0orPFeOELi3CmIVeoCHt5GRyWG7Lz2GOUviYgqXzMl9kJtMeIZcUe4h8mUzNF9bWiW66Mf6dr5eMDilWiJArs41UwTp5nnDWJ08SGiPJ5YjTk5FONnJKgzn5aP3FRMY7ttLfk3cCZ36aEqsBUafLIxzuqDilBBBIRMHM32I3WdmP30bO4nWbgQc/mVxAxYb+tUTIj99GaTM+sZ2cy1nw8uPAnb9rl3FGBin+fPOXo222Z18+L5y87YFPxjv0538I/P1FURrpcp+XXFw4+5jF+PKlx6OnNMWLTbQiWDt3WehA3Gloc2xWO9b2v7ktTUNXufiw2yxDWtoAAG2XXKSGbh7H7bMtfiGjXML9Uzo1q4WeMt1dLmBQ7Iq3ywznyyJz8YhmREx7L3QBH/hfwMXfiEY2FSF5yBFLGB89EH3f/1o8iybLBbUsnG1Ogy9U4qMyAHj4s5EEwdFIfD342ZxzeFTGxFhE7ouOoWNffozmFfCzwoTe53CqHnYKSTMv/IjKsC1G/sgXgH/+g/i20QPAdz6SDNFkTI4lnxFrQIDlvpZn0zUb3EUjgHuvprpxaGb3XGrrHedOXdZIgbYj9FX9Pdh83FIoHrIfSmDt3KWhAwYpNyC5OKNcUjT08mx3p5AgdLGPlFzMGGmbhm6LlWZrNKahp1yDmIVukJYJKbmUut0jH1sbTcgRDaNYoXKLXeRr2PLV+HklzGE+E2b3vGTdXE5RE8NG1E2hEo1eeMYsQB3S0uOj9AFhCojA6JgjViSbGI3klg2XUTK0024ANl4ddBglynSpctGKVbZruPI0AJrCNQFaR+Cnt0ayldaUWfTnd1NWzydup/pt/ydKOsfJvmwwO+GshH7MB6mu37qQLPN8gXwGLLnwfXnp3ynj6U9vzb6sYh1oO8nlkAZbP64oF8CQTZpsoafNJGWLs9IX326+KLap7mGUSzEKzTOXWrNFudhiuNkqjVnoKRZzFgudEZNcqhB6lsiikoXQ5ezf2Usoi+a/p5CQ2fHIVA/m+WuxDE0HXqEioo1EOas2UUcx/DZp6JwB00roIxGJzV8NbLwqfo5KH2n2PQujeQGzl5HTUeLwIMQUmjpgloGe/R5w/l8RobPT9JsfpKUV9zxPaaoXHE1rArzwMPkntn09XrZp+OQyEvqidRRNdf8f0+8XfwPY+ifArmfo9xOvohz/B9+kNRSgyX9w2T+i4RQGFrSdhX5II0uUCxA9nDxLsxZktdAnRuwWeqWvAQvdkFxk1EPCQh9zT7OPaehpFrqMcgn2T1vDVGraxS73yCeL30ISOp+7aLk2afJNgtDZZ1JJ3rdaNHQz0qQoLHSZaXLt+4T0NSo09OAZ7VsetXFiLApZrBgZP4Eo786cFYFkVBIdggjV7V1ESzQC0QpVR55H2T+/egpwZ5BVMl8iMu+aCzz2v2ld0I1X07tw0Z3A2Z9P1sF8Tqtp6BLrtgA37gRu+CWNImTq4lN+n2YGn3wNAE31/fUjwE+/Yi+rQXgLvZ2Q1UIPCbIOx3E1yzZvkSkAIrFckQhdkqlZByuhizh0M7eI7biQ0B2zLKuNNFQ+SAhm1CENhUqUlIudolnOnYZcjsoZOxAM0ffY8/OklWVa0jIU07zmjWi3MQ096Bgu/iYR086HA1lMJSWXSh+lXH78bym3P0sutrVWL7pHymtfAAAWSUlEQVSTFrBYsp7CNY84nZYWBALn6r6o3KUbKOzxrM8Cx11CMf6vPknRUz/5IqVKnr86soI5b8vas0SbLNc5IbnYIrwyRtZJQudjTr4OOPzdlEr653cnk+81CZ7Q2wlZNXR+OGuNQQfiSbKqEbpJguVZFgvd2MdmhUoLPQ1mlEu1zIayLNkmW3mAfYht1rE8OyD0Lve1zRpZVJ4VpRIY2uO+NibMDi+UXCrJY2S7y31xR6wTwXUzI4AOO4XugQwfDecbBMRbnk2zg7vn0flZcrHFqfevjZY2BMhxys949/w4ob/rPxOpV/qoHgAt1bh8I+3Tt5zSKyxeDyw/0d6sXI5IXfoWEk7RlLQXWSD9I/ws5HK0ni8AbLg0Wzl1wBN6OyG00KukXJWSSz0ICd0hudjK7z+SknDlDctdoppTNA22KBeXhW7G0OfLcWJLDLGrWehlCifc/1o0sShLXV0ozQLwRkRyLv9CNYyKMFXXMV0BoRe7q0sxhQp1hiahS5/HxGjUYQJRHDzH3PO+B3bTqMjmO7CBO4bu+cDe5yOSXPUe+rPhtE9E/3NKh9TyTUI3rplVQ894L2IW+vSGV3tCbyfU6hStm9CFBJL4zSFlXP0gfWoxWcV8CayyguN84XHSKVp0a+gyT3tqPTKEqZm/8xT3Yre7rpmH5iJmG6hNcjEhLXTXCIHDIMuzk4Su8lGudEBYl0ZqYTmimhwHTYkPrt/81cDNb0TZQHnfwV0kt2T16YQWeuC7yNoRZEWpJ57yNzFis4RFZx55Bc+Jyk2J49MF7xRtJ9QatljrpKLweIeF75JTlAosulyUZMq0UFyygmux3pjkUnRr6DbrKkHgdUguFUHoTgs943XnyCDWlWuRXEwwEVez0Mt9AFTUFiBqi7Qs5XZzFm14v4pRZkx5Tpnama/zwTdrI+WiSehVjJhaYY6GzOdBjjrCfWrU0KfZOgc8obcXMk8sKsc/a4VL045JLg7iSJNRmiW5uDT0LDHEWeKOzeOlhe7S0Gu10DnywzV6yQqbhh77vUT1l+TN/5uEzh1MLEZfSDCsoU+OpdeTt3O62awoCg0daL6FzuXzcom2+pudfK2EXo8Pq0F4Qm8nhBZ6VsmlRjJgOCUXI3ywWhnmQ10tyiUNpoWuJ8RiFrPj+9os/WoWulLxhQ0SdZQWehPCFoGIpErdFIPduyi5T62ymS0OPVZemZyGckZmGqHzvQrlh2BWr1x/llM0pI1wuC7DA9mW22Pws941RZILEzpLULZ7Vk2mS8MMWuheQ28n8EOd1UKvV3Ip1Cm5xPZLIWmr4y9LlIu00IP9OPa60hfPK2610E3nrK1tRWB8Irmd6xiz0F0Ti2qMhihUgOt+Yg/pa7aFni8CVz1ABPvLf6ZtJTHLN1ZW0A52jE6OJ+//pBG2mKhPUMbwQDxRWzUUhfFSqEyd5FKZQ3KQ9Vk3LfQanaL1vn8NwBN6O2HNmcCmPwEWrnPvl8XizXR8jVEuEmyxJYhUfleI53LJaqEH+0lClyvsWDV09itUgqRhlpctVwTgkHE2XEaWbS5XZep/jRZ6vkQzQ63nzVJWcB2BDJJLOTkSYCJPWOii82VCN9MlTIzCSegxyaUWC53lxS7gnL+MFglpFngEwKMuq4VuynK1WugtKrkopc5RSu1QSu1USt1k+f1KpdRupdRTwd9/an5VPdA1Fzjj5uqec5eFnQUugk2bWJRahiPKJXzwHR0IzxQ0o1yAaDIND5vDc9omhQTXhInC2lkZx8mRUKEMLDwKeNe1yfqYqHVo3ugkpZIR91xNcgHi7TclF5asbPnirVFOOv2cvH1sqDZClyG6G6+Ocrw0C6bkYqt/eB260vexoezoJKYYVQldKZUHcCuAcwGsA3CpUspmIn5Ha70h+Lu9yfX0qAWNSi71RrnE9rOQAkDWLZdhWjK28piATQ0diFvoQPSSuix0Jj/btTGPk0SZFvEgt7PTOuuL76pLeJ4Mw3wpR+TLdO1Vyqtt869wJkmOKOGEYJKAWUePjdBEB5gWoSSt2lqcohzKKbNNNhMhoQcyl+tZD6WxWqNc6vRhNYAsFvrJAHZqrX+ttR4FcDeALVNbLY+GMJWSSy6P0GrOJLnYyLUSEI9BirbyQkI3olyAiND5BQolDIeGHu6TwQlWtuRbkW2Q5cXqWquG3miir56ojnkRgSKhjLhw+ftxl9LKSrxKUvc8AMreiaY5xVMtdHFNa7HQFx9LqwMd8d7sx9SCYjddE5feHaYzENJYFriesSlGFkJfBkAuAf5KsM3E7ymlnlZK3auUsiY1Vkpdq5TappTatnv3btsuHs3AVEouMj43U9ii5aEuVKJUsUA8DM5ENQu9UBHk6pBTChmsLa6HuVhDvpycEGOWp3IRYdVsoWeQXMKOQ9SDSdrV6ZnnkjHkjPLs+MIYnLfGzHBplivLSNXQ5QimBgtdKcq/Ypvg0wwceQ5w8rXuZ7leQp9Bp2izwha/B2Cl1no9gB8AuMu2k9b6Nq31Rq31xgULFth28WgGwhezTkKvNtM0ixMzzVIEAjKvJMuxDdttpBcj9HKS0K3TtrNY6AZJu5xbpoWeK0YvcNap/1lefJNUJCmyXGArJxFd1B3fJzb5yxjRFbtoNSG54LNtxCUll0yEXoOFPtVYtQk49wvC+HFIcOU6CX0GnKJZolxeBSAt7uXBthBaa7GAHm4H8MXGq+ZRN1wPaRZUs8ALJWDU8bssw9apMOGZIwmrhc5EZnGKhhZ6Bocn7+Oy0EPy7I3PbHRNVCoJ3bxW30XoPHMQnexkBkGkyGlsw0yEtk7PtNC7k9s5TNN0JhfKwNUPxJ+fUENPkVnSHOSFFiV0RjhfwtHBZ/F1SOTyQUbO1rTQnwCwVim1SilVAnAJgPvkDkopGXO1GcCzzauiR81oeOp/Eyx0W1QEo1AhEjM7jlo19NHBuIUeOkUdqU9dFjofF1rojhfZtNDzxdp9F8tPAs7+c3ciKbPjkA5Qdhy6ZCmGzWFrjoyk5FLuhXWhFFvki1murf5AbZLLdMEZABBcF5ehkIZyb2ta6FrrcaXU9QAeApAHcIfW+hml1OcAbNNa3wfgY0qpzQDGAewDcOUU1tmjGrJo3JmOr+LoyjSxyEGGCaeoK8olRXIp9SRXy3FZ6C5ry5Q3arH48yUhuWTsSPMF4N3XV9lHjBqAOCkmCN3S6ZV6KRNjyZBcgCSRS0I3Ee6bopunhdLW6xSdLmSZRFfPzM81ZwKL3tlY3epApolFWuutALYa224R/38KwKeaWzWPujHVkksmDd1B+t3zKXVpFsmlZwFtl2QgCb17HhEbL64BuFOflhyhhaZmWnJooZIw+XuW61IrEqFzZXKG6kla8V62W5J12IF1E6EXbZKLOUJiK9xF6Clhq6mdf51hi9MFlzQYPg91aOIXfLWxetUJn8ulE9GoUzSr5JJlYpHNWj3/S8DmrySJhD9luRuvAq75sUHoHLb4NunQx10KXPtjMUnE4RR1ySimhV52yDNhhkLepxDp6M1cwDzhHxAdx4lXUrvD9LvGKKZQSS4NZyPkhOTikKNq1dBb1SnKcEVjmUbADCTbqhWe0DsRjVqK1ZJ7ZZFcbEN0xuyltH6kqV+aLxBA1tFiY+gam+U4m4hi8bFJgpIIidFhbYUaeuCsdDlFzYlKLLk02xGWCJ0rCstxdrzdZpRLoRxJIWaUiyzb7KCdC3lLEs8S5SIllxa00F0RXfWGLc4gPKF3IlpCcnFYPow0yUUSus3yk9tkXm/XZCabdJHYJziubzk5bfvfET82VnfTQi/GredmIaHjFpGYhcvtNjV0aaH3Lafvc1dG+5jXyxXx4Zz6j/RnRZbVkhY6+z1cGnr7ELpPztWJaFhyqRKtUZOGXoPOHsZBdyf3sZ0fiGcIzBsTgyS4k+D9XRp672LgT1+N1su0EZxt6nixu/riI7WC21+2afVGXhbTQs+Xo99mLwX+9DVjYWxHlIsJ69T/WqNcWpDQnXHowXXhe9yKIwwDntA7EY1KLpnDFjNEuTgXgkiTXMSLYw1BlJJLb3JfW8TFsR8iB2vv4qBeNgtdyEQx4rO0c+7hwIW3A2vfB9x/A7XhPTfQSvTNhDlqyQnJxZxpm7DQBaHnCslZl/kClaWMVA6unPVpoZFpGnouHzlxW5EQXYYHbzvid4ALvgasOHm6alU3vOTSiWh46r8lI1/s9wyE7opDZ5j6ZajhymRTjogVwC652Milay5wzAXiBc4gK+TyNJsyrQ3rL4qst1yB1tNMW8C4XiTi54tJcrU5M9ecCRx9vrgmKbH5MdnE0jEwqk79d9iGro5ipuGUXIRPYcOHpy4NQRPhLfROBC8k0FNneoWefnoJzfzYjExySgYN3ew4JDHxgsWuSUKAIblkcNZ2BYmnevot5VrIr5ouzrltpkpf5cyHPA0/X0peL1u44cnX0Ocd59Cn7TrminEizhTlkiKzOJ+FMuWgb0ULfdZC+rS9K64cQy0KT+idiHmrgD/8GTDviPqOX3cBsOzEKBzORCbJxRHlwkg4RQVR5UsUq17VKSrSq7oiaxhzVgB/+KT92tg6qixkPZWE3r8GuP5JcmpyHXOBA5alElv+cobrmrAjN/zOPgyL1h2WU6PkIvdrRQ19+Ua6vv1rkr+ZPoY2QPvU1KM2zF9d/7H5AnUKqb/XEOXiCuM75kKEmRdj5QaywsQIYivIh2UbYYsMM1tiGtKujTUBVbE6WecK7k6kUfSvASYnRX2KdmJ1ad82uUCGQALA/DXAphuBNWell5PqFHURuqOjaAXYyBwA1l9MFnwz5xVMMTyhe9QO2wQgEwuOAhYc7SbD/jVA/8eS5bKskGWySqVGycUF2/FL1gMLj65yXGlqCR2gji1XiM5ly6niCje0zp416p3LA2f8t5Tz15k+FyB9Olec+mvUbCw8uvq9bzF4QveoHVkkl2M/RH81lSsI3UUA1aJcqlnoabAd/5HvVj+uPKv5q9LbUJpF0S65oj3axGmh21IT92Svt03Wyayhl1pTP+9AeEL3qB2NhkVWLTeQOdK0SxkKZ3WK1vlYZ3H22vChv7M7WZuNy+6lcMlXf2aQaQZCt12T995MizdngdUpmmEJOiAg9BaVWzoMntA9ake11AD1QnYU+Sq6dL5EkROVOjT0NOTq7BCWbqjvfLVixUn0WZfkYmnT3MOznzvNYWz7P3GsJ/Tpgid0j9rRuxToWdj8uFxpBbosdCAgqpEoN0u4DU3Q0Fs8TK13CY1OGPVKLrWgEQ3dSy7TBk/oHrXjpI82f0YkEMR0lyMNvVooXLk3HgUTTv2v87Fu1MKfLpz334HJiei7c8p+k9pkJXReMFy7r3mhTHMKPKYcntA9ake+CHTNmZqyt3yFYuBf/De39GGz+ppmobc4oZvyxaJjgLM+T7NDTTTLQreNXpSi7Vq7Q/tO/yQwOdbY+T0ywRO6R2th/cX06Qpb5N/NmaxN09BbnNBN5PLAqR+z/xb6JaZAcuHvWruPXXlqY+f2yAxP6B6tiWqx3TZCb9TCrvRR9Mx0hCBOF3INylBhOazTG4SeKwCoQuge0wZP6B6tiVyhulNUhizyMfKzVrzzQmDBkbSsXacgyySwLAjT51osdE/oLQNP6B6tiRMuB0YH039/13XJhEoLjqLjDq9ziF8oA8tOqO/YVkUYtthgRFLa6Ic1dI+WgCd0j9bEOy90/37SR5PbihVg819PTX3aFVmSpGWBLTmX3O7REvB3w8Ojk+GaWFRTOQ6nqJdcWgae0D08OhnNSgGblobXSy4tBU/oHh6djBUnA6vPSM9tnxXLTgRWbaJZqhKe0FsKmZagU0qdo5TaoZTaqZS6ybHf7ymltFJqY/Oq6OHhUTeWb6SMkY1q6EvWA1d8L8pdz2jHtLgdjKqErpTKA7gVwLkA1gG4VCm1zrJfL4CPA3is2ZX08PBoUUzlak0eNSOL5HIygJ1a618DgFLqbgBbAPzS2O/zAL4A4Mam1tDDw6N18e7rZ7oGHgJZJJdlAF4W318JtoVQSp0AYIXW+n5XQUqpa5VS25RS23bv3l1zZT08PFoMR/0u/Xm0BDJp6C4opXIAvgTgj6vtq7W+TWu9UWu9ccGCOlek9/Dw8PCwIguhvwpghfi+PNjG6AXwTgCPKKVeBHAKgPu8Y9TDw8NjepGF0J8AsFYptUopVQJwCYD7+Eet9YDWul9rvVJrvRLAowA2a623TUmNPTw8PDysqEroWutxANcDeAjAswDu0Vo/o5T6nFJq81RX0MPDw8MjGzJNLNJabwWw1dh2S8q+v9N4tTw8PDw8akXDTlEPDw8Pj9aAJ3QPDw+PDoEndA8PD48OgdIzlFhHKbUbwP+r8/B+AHuaWJ1WRKe30bev/dHpbWzV9h2utbZO5JkxQm8ESqltWuuOjnPv9Db69rU/Or2N7dg+L7l4eHh4dAg8oXt4eHh0CNqV0G+b6QpMAzq9jb597Y9Ob2Pbta8tNXQPDw8PjyTa1UL38PDw8DDgCd3Dw8OjQ9B2hJ51fdN2glLqRaXUL5RSTymltgXb5imlfqCUej74bHCV3+mFUuoOpdQupdR2sc3aJkX4cnBPnw4WTGlppLTvM0qpV4P7+JRS6jzx26eC9u1QSr1/ZmqdHUqpFUqpHyulfqmUekYp9fFge0fcQ0f72vseaq3b5g9AHsALAI4AUALwcwDrZrpeTWjXiwD6jW1fBHBT8P9NAL4w0/WssU2bAJwAYHu1NgE4D8ADABQon/5jM13/Otv3GQD/1bLvuuBZLQNYFTzD+ZluQ5X2LQFwQvB/L4DngnZ0xD10tK+t72G7Wejh+qZa61EAvL5pJ2ILgLuC/+8CcMEM1qVmaK1/AmCfsTmtTVsAfEMTHgUwRym1ZHpqWh9S2peGLQDu1lqPaK1/A2An6FluWWitf6u1/lnw/35Q6uxl6JB76GhfGtriHrYboVdd37RNoQH8i1LqSaXUtcG2RVrr3wb/vw5g0cxUralIa1Mn3dfrA8nhDiGTtXX7lFIrARwP4DF04D002ge08T1sN0LvVJymtT4BwLkA/otSapP8UdOYr6PiSzuxTQC+BmA1gA0Afgvgf8xsdRqHUmoWgH8E8Amt9dvyt064h5b2tfU9bDdCr7a+aVtCa/1q8LkLwHdBQ7k3eMgafO6auRo2DWlt6oj7qrV+Q2s9obWeBPC3iIbkbdk+pVQRRHbf1lr/U7C5Y+6hrX3tfg/bjdCd65u2I5RSPUqpXv4fwNkAtoPadUWw2xUA/s/M1LCpSGvTfQAuDyIlTgEwIIb1bQNDM/4g6D4C1L5LlFJlpdQqAGsBPD7d9asFSikF4OsAntVaf0n81BH3MK19bX8PZ9orW+sfyJv+HMjLfPNM16cJ7TkC5D3/OYBnuE0A5gN4GMDzAH4IYN5M17XGdv0DaMg6BtIbP5rWJlBkxK3BPf0FgI0zXf862/fNoP5Pgwhgidj/5qB9OwCcO9P1z9C+00ByytMAngr+zuuUe+hoX1vfQz/138PDw6ND0G6Si4eHh4dHCjyhe3h4eHQIPKF7eHh4dAg8oXt4eHh0CDyhe3h4eHQIPKF7eHh4dAg8oXt4eHh0CP4/pulwGt6XKaUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle as pkl\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(252,252,1)),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu'),\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(100,activation='relu'),\n",
        "                                    tf.keras.layers.Dropout(0.5),\n",
        "                                    tf.keras.layers.Dense(2,activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.load_weights('/content/drive/MyDrive/Introduction to Bioinformatics/Models/H3K4me2_3000_26_December_59_5.h5')"
      ],
      "metadata": {
        "id": "99YDf0-zOR8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins = SeqEmbedding()\n",
        "X_test = ins.fit(sequences=X_test,window_size = 3, stride_size = 1)"
      ],
      "metadata": {
        "id": "ahn-1ymiQOZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.shape)\n",
        "X_test = X_test.reshape(X_test.shape[0],252, 252, 1)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AHiNwAeQoY6",
        "outputId": "3ab1cc34-dea2-446c-ceb4-1fd753e88e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2, 31752)\n",
            "32/32 [==============================] - 9s 28ms/step - loss: 0.6202 - accuracy: 0.6980\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6202206015586853, 0.6980000138282776]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n",
        "p = model.predict(X_test)\n",
        "prediction = []\n",
        "for x in p:\n",
        "    if(x[0]>x[1]):\n",
        "        prediction.append(0)\n",
        "    else:\n",
        "        prediction.append(1)\n",
        "print(f'Accuracy: {accuracy_score(y_test,prediction)}')\n",
        "print(f'Precision: {precision_score(y_test,prediction)}')\n",
        "print(f'Recall: {recall_score(y_test,prediction)}')\n",
        "print(f'F1 Score: {f1_score(y_test,prediction)}')\n",
        "print(f'MCC Score: {matthews_corrcoef(y_test,prediction)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suYX8qHbjFmh",
        "outputId": "6672051f-4808-4e71-9b1d-26b35d3131ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.698\n",
            "Precision: 0.7061340941512125\n",
            "Recall: 0.8375634517766497\n",
            "F1 Score: 0.7662538699690402\n",
            "MCC Score: 0.35856836322113667\n"
          ]
        }
      ]
    }
  ]
}